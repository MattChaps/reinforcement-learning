\documentclass{article}

\usepackage{biblatex}

\addbibresource{references.bib}

\title{Reinforcement Learning: A Literature Survey}
\author{Matthew Chapman}

\begin{document}
\maketitle

\section{Introduction}

Reinforcement learning is the class of problems concerned with an agent learning behaviour through trial-and-error interactions with a dynamic environment \cite{Kaelbling1996}.

An example is an aspiring tightrope artist (the agent) learns to walk
from one end of a rope to another without falling (the behaviour) by
repeatedly correcting their balance (the trial-and-error interactions) whenever the rope wobbles beneath them (the dynamic environment).

In this literature survey we: 
\begin{enumerate}
  \item{describe the problems we want to solve with reinforcement learning, and explain why they are interesting to solve;} 
  \item{describe historical and current work in the field, including the kinds of problems solved and the approaches; as well as,} 
  \item{state what we will be using in our own project.}
\end{enumerate}

\section{Motivation}

A primary goal of some research is the development of artificial general intelligence (AGI) \cite{}, where intelligence is defined as the ``adaptation with insufficient knowledge and resources'' \cite{OnDefiningArtificialIntelligence}. Reinforcement learning is itself an area of artificial intelligence (AI), as it is a technique humans and other animals use to understand their environment and generalise past experience to new situations \cite{Mnih2015}. Therefore, to study reinforcement learning is to study how a machine can be made to learn how humans do. This is interesting because it allows machines to learn and solve complex problems for us by themselves

\section{Historical Work}

\subsection{Markov Decision Processes}

Markov Decision Processes (MDPs) are a mathematically idealised form of the reinforcement learning problem. In a MDP, the agent senses the state of its environment, selects an action, which changes the state of the environment, and receives a reward from the environment \cite{Sutton1998}. The goal of the agent is to maximise the total reward received by the environment over time.

\subsection{The Gambler's Problem (Dynamic Programming)}

\subsection{Blackjack (Monte Carlo Methods)}

\subsection{Backgammon (Temporal-Difference Learning)}

"TD-Gammon is a neural network that trains itself to be an evaluation function for the game of backgammon by playing 

\subsection{(Policy Gradient Methods)}

\section{Current Work}

\subsection{(Q-Learning)}
Q-learning is a value-based class of algorithms that aim to build a value function, which subsequently lets us define a policy.

$Q$-learning keeps a lookup table of values $Q(s,a)$ with one entry for every state-action pair. In order to learn the optimal $Q$-value function, the $Q$-learning algorithm makes use of the Bellman equation for the $Q$-value function (Bellman and Dreyfus,1962) whose unique solution is $Q^*(s,a)$.

This is often inapplicable with a high-dimensional state-action space.

\subsection{Atari 2600 (Deep Q-networks)}

The DQN algorithm obtains strong performance in an online setting for a variety of ATARI games, directly by ;earning from the pixels.

Rewards are clipped between $-1$ and $+1$.

\section{Our Project}

Pull it all together, and say what we are taking to use in the project

In this project, we want to look at deep reinforcement learning algorithms, implement them, and analyse their performance in those environments provided by OpenAI Gym.

\printbibliography
\end{document}