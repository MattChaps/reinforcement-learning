# 1.5 An Extended Example: Tic-Tac-Toe

- Cannot be solved through classical techniques such as "minimax". Best can do is to learn a model of the opponent's behaviour, up to some level of confidence, then apply dynamic programming to compute an optimal solution.

- Evolutionary model searches space of possible policies (rule that tells player what move to make for every state of the game) for one with high probability of winning. For each policy, obtain estimate of winning probability by playing some number of games against the opponent.

- Tic-tac-toe approach with a method using a value function:

  1. Set up table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of winning from that state &mdash; the state's _value_. The whole table is the learned value function.

    - State A has higher value than state B (better), if estimate from A > that from B.

    - Assuming playing Xs:

      - &forall; states with 3 Xs in a row, __P__[win] = 1

      - &forall; states with 3 Os in a row, or that are filled up, __P__[win] = 0

      - Set initial values of all other states to 0.5.

  2. Play many games against the opponent.

    - Select moves by examining the states that would result from each possible move and look up their current values in the table. Most of the time move _greedily_; occasionally select randomly. These are _exploratory_ moves because they cause us to experience states that we might otherwise never see.

While playing change values of states which we find ourselves in during the game. Attempt to make more accurate estimates by "backing up" the value of the state after each greedy move to the state before the move: current value of earlier state updated to be closer to the value of the later state.

Let S<sub>t</sub> denote the state before the greedy move, and S<sub>t+1</sub> the state after that move, then the update to the estimated value of S<sub>t</sub>, denoted V(S<sub>t</sub>), can be written as

<img src="https://render.githubusercontent.com/render/math?math=
V(S_t) \leftarrow V(S_t) %2B \alpha[V(S_{t%2B1}) - V(S_t)]">

where alpha is a small positive fraction called the _step-size parameter_, which influences the rate of learning. This updated rule is an example of a _temporal-difference_ learning method because its changes are based on a difference between estimates at two successive times.

- If step-size parameter reduced properly over time, then method converges, for any fixed opponent, to the true probability of winning from each state given optimal play by our player. If not reduced all the way to zero over time, also plays well against opponents that slowly change their way of playing.

- To evaluate policy, evolutionary methods holds policy fixed and plays many games against opponent or simulates many games using model. Frequency of wins give unbiased estimate of the probability of winning with that policy, and can be used to direct the next policy selection. Only final outcome of each game is used. In contrast, value function methods allow individual states to be evaluated.  Both search space of policies, but learning a value function takes advantage of information available during the course of play.

- In the example no prior knowledge beyond rules of the game. Prior information can be incorporated.

- Model-free systems cannot think about how environments change in response to actions. Tic-tac-toe player is model-free wrt opponent: has no model of opponent of any kind.
