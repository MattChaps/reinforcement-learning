# 1.5 An Extended Example: Tic-Tac-Toe

- Cannot be solved through classical techniques such as "minimax". Classical optimisation methods for sequential decision problems, such as dynamic programming, can _compute_ an optimal solution for any opponent, but require as input a complete specification of that opponent, including the probabilities with which the opponent makes each move in each board state. Best can do is to learn a model of the opponent's behaviour, up to some level of confidence, then apply dynamic programming to compute an optimal solution.

- Evolutionary model searches space of possible policies (rule that tells player what move to make for every state of the game) for one with high probability of winning. For each policy, obtain estimate of winning probability by playing some number of games against the opponent.

- Tic-tac-toe approach with a method using a value function:

  1. Set up table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of winning from that state &mdash; the state's _value_. The whole table is the learned value function.

    - State A has higher value than state B (better), if estimate from A > that from B.

    - Assuming playing Xs:

      - &forall; states with 3 Xs in a row, __P__[win] = 1

      - &forall; states with 3 Os in a row, or that are filled up, __P__[win] = 0

      - Set initial values of all other states to 0.5.

  2. Play many games against the opponent.

    - Select moves by examining the states that would result from each possible move and look up their current values in the table. Most of the time move _greedily_; occasionally select randomly. These are _exploratory_ moves because they cause us to experience states that we might otherwise never see.

While playing change values of states which we find ourselves in during the game. Attempt to make more accurate estimates by "backing up" the value of the state after each greedy move to the state before the move: current value of earlier state updated to be closer to the value of the later state. Let S<sub>t</sub> denote the state before the greedy move, and S<sub>t+1</sub> the state after that move, then the update to the estimated value of S<sub>t</sub>, denoted V(S<sub>t</sub>), can be written as

<img src="https://render.githubusercontent.com/render/math?math=
V(S_t) \leftarrow V(S_t) %2B \alpha[V(S_{t%2B1}) - V(S_t)]">

where alpha is a small positive fraction called the step-size parameter, which influences
the rate of learning.
