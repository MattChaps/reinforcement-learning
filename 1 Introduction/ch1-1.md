## 1.1 Reinforcement Learning

- RL is learning what to do &mdash; how to map situations to actions &mdash; so as to maximise a numerical reward signal. The learner discovers which actions yield the most reward by trying them &mdash; trial-and-error search. Actions may affect immediate reward and all subsequent rewards &mdash; delayed reward.

- Reinforcement learning is a problem, a class of solution methods that work well on the problem, and the field that studies the problem and its solution methods.

- The problem of reinforcement learning is the optimal control of incompletely-known Markov decision processes. A learning agent must be able to sense the state of its environment to some extent and must be able to take actions that affect the state with a goal relating to the state of the environment. A method well suited to solving such problems is a reinforcement learning method.

- RL is different from supervised learning, which is learning from a training set of labeled examples. Each example is a description of a situation with a specification &mdash; the label &mdash; of the correct action the system should take in that situation (e.g. identify a category to which the situation belongs). Object is to extrapolate to act correctly in situations not present in training set. Not adequate for learning from its own experience.

- RL is different from unsupervised learning, which is finding structure hidden in collections of unlabeled data. RL tries to maximise reward signal. RL is the third machine learning paradigm.    

- A challenge in RL is trade-off between exploration and exploitation. To obtain a lot of reward, a RL agent must prefer actions it has tried in the past and found to be effective in producing reward. But to discover such actions, it has to try actions not selected before. The agent has to _exploit_ what it has already experienced in order to obtain reward, but it also has to _explore_ to make better action selections in the future. Neither can be pursued exclusively without failing at the task. Agent must try a variety of actions _and_ progressively favour those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward. This is an unresolved dilemma and does not appear in supervised and unsupervised learning.

- RL explicitly considers the _whole_ problem of a goal-directed agent interacting with an uncertain environment.

- RL starts with complete, interactive, goal-seeking agent. All RL agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments. Agent has to operate despite uncertainty about the environment. When RL involves planning, it must address the interplay between planning and real-time action selection, as well as how environment models are acquired and improved.

- Complete, interactive, goal-seeking agent does not always mean a complete organism or robot. Can be a component of a larger behaving system: agent directly interacts with rest of the larger system and indirectly with the larger system's environment &mdash; e.g. agent monitoring charge level of robot's battery and sends commands to robot's control architecture.  

- RL part of trend within AI and ML toward greater integration with statistics, optimisation, and other mathematical subjects. RL closest to human learning.

- RL also part of trend in AI back toward simple general principles of learning, search, decision making. Since late 1960s researchers presumed that getting enough facts into a machine would lead it to become intelligent.
