## 1.3 Elements of Reinforcement Learning

- Beyond agent and environment, 4 main sub-elements: _policy_, _reward signal_, _value function_, and, optionally, _model_ of the environment.

- Policy: defines learning agent's way of behaving at a given time.

  - Mapping from perceived states of the environment to actions to be taken when in those states.

- Reward signal: defines goal of a RL problem.

  - On each time step, environment sends to RL agent a single number called _reward_ (immediate).

  - Objective of agent is to maximise total reward it receives over the long run.

  - Reward signal defines what are the good and bad events (immediate).

  - Reward signal is primary basis for altering policy.

- Value function: specifies what is good in the long run.

  - Value of a state: the total amount of reward an agent can expect to accumulate over the feature, starting from that state.

- Without reward then no values, and only point of estimating values is to achieve more reward. Nevertheless, most concerned with value when making and evaluating decisions. Most important component of almost all RL algorithms is method for efficiently estimating values.

- Model: mimics behaviour of the environment and allows inferences to be made about how the environment will behave.

  - Given state and action, model might predict resultant next state and next reward.

  - Used for planning: way of deciding on a course of action.

  - Methods for solving RL problems that use models and planning are called _model-based_ methods, as opposed to simpler _method-free_ methods that are explicitly trial-and-error learners (the opposite of planning).
