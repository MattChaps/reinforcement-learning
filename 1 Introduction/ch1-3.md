## 1.3 Elements of Reinforcement Learning

- Beyond agent and environment, 4 main sub-elements: _policy_, _reward signal_, _value function_, and, optionally, _model_ of the environment.

- Policy: defines learning agent's way of behaving at a given time.

  - Mapping from perceived states of the environment to actions to be taken when in those states.

- Reward signal: defines goal of a RL problem.

  - On each time step, environment sends to RL agent a single number called _reward_ (immediate).

  - Objective of agent is to maximise total reward it receives over the long run.

  - Reward signal defines what are the good and bad events (immediate).

  - Reward signal is primary basis for altering policy.

- Value function: specifies what is good in the long run.

  - Value of a state: the total amount of reward an agent can expect to accumulate over the feature, starting from that state.  

    - indicates the _long-term_ desirability of states after taking into account states that are likely to follow and the rewards available in those stages.

- Rewards primary, whereas values, as predictions of rewards, are secondary. Without reward then no values, and only point of estimating values is to achieve more reward. Nevertheless, most concerned with value when making and evaluating decisions as action choices are made based on value judgements. Seek actions that bring states of higher value, not highest reward, because these obtain greatest amount of reward over the long run. Values must be estimated and re-estimated from sequences of observations an agent makes over its entire lifetime. Most important component of almost all RL algorithms is method for efficiently estimating values.

- Model: mimics behaviour of the environment and allows inferences to be made about how the environment will behave.

  - Given state and action, model might predict resultant next state and next reward.

  - Used for planning: way of deciding on a course of action.

  - Methods for solving RL problems that use models and planning are called _model-based_ methods, as opposed to simpler _method-free_ methods that are explicitly trial-and-error learners (the opposite of planning).
