{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O7cSmYCTkET2","colab_type":"text"},"source":["# Chapter 2 "]},{"cell_type":"markdown","metadata":{"id":"PbqtrpgbkJt4","colab_type":"text"},"source":["## Multi-armed Bandits\n","\n","- RL uses training information that _evaluates_ the actions taken rather than _instructs_ by giving correct actions. Evaluative feedback depends on the action taken, whereas instructive feedback is independent of the action take.\n","\n","- In this chapter study evaluative aspect of RL in _nonassociative_ setting, which does not involve learning to act in more than one situation."]},{"cell_type":"markdown","metadata":{"id":"iZcYk74GkNuW","colab_type":"text"},"source":["## 2.1 A k-armed Bandit Problem\n","\n","- Consider the learning problem:\n","\n","  - faced repeatedly with choice among k different options;\n","  - after each choice receive numerical reward chosen from a stationary probability distribution that depends on the action you selected\n","\n","- Objective is to maximise expected total reward over some number of time steps.\n","\n","- This is original form of k-armed _bandit problem_, so named by analogy to a slot machine with k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and rewards are payoffs for hitting jackpot. Through repeated action selections you are to maximise your winnings by concentrating your actions on the best levers.   \n","\n","- Each of the k actions has an expected or mean reward given that that actions is selected -- _value_ of that action.\n","\n","- Let action selected on time step _t_ as A<sub>t</sub>, and the corresponding reward as R<sub>t</sub>. The value then of an arbitrary action _a_, denoted q<sub>*</sub>(a), is the expected reward given that _a_ is selected:\n","\n","$$\n","q_*(a) \\doteq \\mathbb{E}[R_t|A_t=a]\n","$$\n","\n","- Assume we do not know the action values with certainty, although may have estimates. Denote the estimated value of action _a_ at time step _t_ as Q<sub>t</sub>(a). We would like Q<sub>t</sub>(a) to be close to q<sub>*</sub>(a).\n","\n","- If maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. Call these the _greedy_ actions. When select one of these actions, say _exploiting_ current knowledge of the values of the actions. Otherwise, _exploring_.\n","\n","- Whether it is better to explore or exploit depends on precise values of estimates, uncertainties, and number of remaining steps.\n","\n","- In this book do not worry about balancing exploration and exploitation in sophisticated way; worry only about balancing them at all"]},{"cell_type":"markdown","metadata":{"id":"Hyu5yZGYkXB8","colab_type":"text"},"source":["## 2.2 Action-value Methods\n","\n","- _Action-value methods_ are methods for estimating the values of actions and for using the estimates to make action selection decisions.\n","\n","- The true value of an action is the mean reward when that action is selected\n","\n","$$\n","Q_t(a) \\doteq \\frac{\\text{sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$ taken prior to $t$}}\n","$$\n","\n","- If the denominator is zero, then instead define _Q<sub>t</sub>(a)_ converges to _q<sub>*</sub>(a)_. This is the _sample-average_ method.\n","\n","- Simplest action selection rule is to select one of the actions with the highest estimated value. Write this _greedy_ action selection method as:\n","\n","$$\n","\\DeclareMathOperator*{\\argmax}{arg\\,max}\n","A_t \\doteq \\argmax_x Q_t(a)\n","$$\n","\n","where argmax<sub>a</sub> denotes action a for which the expression that follows is maximised. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability epsilon, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. Methods using this near-greedy action selection rule are epsilon-greedy methods. An advantage is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Q<sub>t</sub>(a) converge to their respective q<sub>*</sub>(a). The probability of selecting the optimal action converges to greater than 1 - epsilon, that is, to near certainty. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M-zZH573-agc"},"source":["## 2.3 The 10-armed Testbed\n","\n","To assess relative effectiveness of greedy and $\\epsilon$-greedy action-value methods, compare them numerically on test problems.\n","\n","Compare greedy method with two $\\epsilon$-greedy methods ($\\epsilon=0.01$ and $\\epsilon=0.1$).\n","\n","For Average Reward against Steps, greedy improved slightly faster in the beginning, but then leveled off at a lower level. The $\\epsilon$-greedy methods eventually performed better because they continued to explore.\n","\n","Advantage of $\\epsilon$-greedy over greedy methods depends on the task. For example, with noisier rewards it takes more exploration to find the optimal action, and $\\epsilon$-greedy methods should fare even better relative to the greedy method."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7d9XZaI-oq_I"},"source":["### Exercise 2.2: Bandit example\n","\n","Consider a $k$-armed bandit problem with $k=4$ actions, denoted 1, 2, 3, and 4. \n","\n","Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a)=0$, for all $a$. \n","\n","Suppose the initial sequence of actions and rewards is $A_1=1$, $R_1=-1$, $A_2=2$, $R_2=1$, $A_3=2$, $R_3=-2$, $A_4=2$, $R_4=2$, $A_5=3$, $R_5=0$. \n","\n","On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. \n","\n","1) On which time steps did this definitely occur? \n","\n","2) On which time steps could this possibly have occurred?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qgQwMI3ooqwb"},"source":["### Answer:\n","\n","Build a table for $Q_t(a)$ for each time step $t$:\n","\n","|     | a=1 | a=2 | a=3 | a=4 |\n","|-----|-----|-----|-----|-----|\n","| t=1 | 0   | 0   | 0   | 0   |\n","| t=2 | -1  | 0   | 0   | 0   |\n","| t=3 | -1  | 1   | 0   | 0   |\n","| t=4 | -1  | -0.5| 0   | 0   |\n","| t=5 | -1  | 0.33| 0   | 0   |\n","\n","- $A_1=1$: random or greedy\n","- $A_2=2$: random or greedy\n","- $A_3=2$: random or greedy\n","- $A_4=2$: definitely $\\epsilon$\n","- $A_5=3$: definitely $\\epsilon$"]},{"cell_type":"markdown","metadata":{"id":"YGjFhqrkkBXD","colab_type":"text"},"source":["### Exercise 2.3\n","\n","In the comparison shown in Figure 2.2, which method will performIn the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."]}]}