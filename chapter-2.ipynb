{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O7cSmYCTkET2","colab_type":"text"},"source":["# Chapter 2 "]},{"cell_type":"markdown","metadata":{"id":"PbqtrpgbkJt4","colab_type":"text"},"source":["## Multi-armed Bandits\n","\n","- RL uses training information that _evaluates_ the actions taken rather than _instructs_ by giving correct actions. Evaluative feedback depends on the action taken, whereas instructive feedback is independent of the action take.\n","\n","- In this chapter study evaluative aspect of RL in _nonassociative_ setting, which does not involve learning to act in more than one situation."]},{"cell_type":"markdown","metadata":{"id":"iZcYk74GkNuW","colab_type":"text"},"source":["## 2.1 A k-armed Bandit Problem\n","\n","- Consider the learning problem:\n","\n","  - faced repeatedly with choice among k different options;\n","  - after each choice receive numerical reward chosen from a stationary probability distribution that depends on the action you selected\n","\n","- Objective is to maximise expected total reward over some number of time steps.\n","\n","- This is original form of k-armed _bandit problem_, so named by analogy to a slot machine with k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and rewards are payoffs for hitting jackpot. Through repeated action selections you are to maximise your winnings by concentrating your actions on the best levers.   \n","\n","- Each of the k actions has an expected or mean reward given that that actions is selected -- _value_ of that action.\n","\n","- Let action selected on time step _t_ as A<sub>t</sub>, and the corresponding reward as R<sub>t</sub>. The value then of an arbitrary action _a_, denoted q<sub>*</sub>(a), is the expected reward given that _a_ is selected:\n","\n","$$\n","q_*(a) \\doteq \\mathbb{E}[R_t|A_t=a]\n","$$\n","\n","- Assume we do not know the action values with certainty, although may have estimates. Denote the estimated value of action _a_ at time step _t_ as Q<sub>t</sub>(a). We would like Q<sub>t</sub>(a) to be close to q<sub>*</sub>(a).\n","\n","- If maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. Call these the _greedy_ actions. When select one of these actions, say _exploiting_ current knowledge of the values of the actions. Otherwise, _exploring_.\n","\n","- Whether it is better to explore or exploit depends on precise values of estimates, uncertainties, and number of remaining steps.\n","\n","- In this book do not worry about balancing exploration and exploitation in sophisticated way; worry only about balancing them at all"]},{"cell_type":"markdown","metadata":{"id":"Hyu5yZGYkXB8","colab_type":"text"},"source":["## 2.2 Action-value Methods\n","\n","- _Action-value methods_ are methods for estimating the values of actions and for using the estimates to make action selection decisions.\n","\n","- The true value of an action is the mean reward when that action is selected\n","\n","$$\n","Q_t(a) \\doteq \\frac{\\text{sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$ taken prior to $t$}}\n","$$\n","\n","- If the denominator is zero, then instead define _Q<sub>t</sub>(a)_ converges to _q<sub>*</sub>(a)_. This is the _sample-average_ method.\n","\n","- Simplest action selection rule is to select one of the actions with the highest estimated value. Write this _greedy_ action selection method as:\n","\n","$$\n","\\DeclareMathOperator*{\\argmax}{arg\\,max}\n","A_t \\doteq \\argmax_x Q_t(a)\n","$$\n","\n","where argmax<sub>a</sub> denotes action a for which the expression that follows is maximised. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability epsilon, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. Methods using this near-greedy action selection rule are epsilon-greedy methods. An advantage is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Q<sub>t</sub>(a) converge to their respective q<sub>*</sub>(a). The probability of selecting the optimal action converges to greater than 1 - epsilon, that is, to near certainty. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M-zZH573-agc"},"source":["## 2.3 The 10-armed Testbed\n","\n","To assess relative effectiveness of greedy and $\\epsilon$-greedy action-value methods, compare them numerically on test problems.\n","\n","Compare greedy method with two $\\epsilon$-greedy methods ($\\epsilon=0.01$ and $\\epsilon=0.1$).\n","\n","For Average Reward against Steps, greedy improved slightly faster in the beginning, but then leveled off at a lower level. The $\\epsilon$-greedy methods eventually performed better because they continued to explore.\n","\n","Advantage of $\\epsilon$-greedy over greedy methods depends on the task. For example, with noisier rewards it takes more exploration to find the optimal action, and $\\epsilon$-greedy methods should fare even better relative to the greedy method."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7d9XZaI-oq_I"},"source":["### Exercise 2.2: Bandit example\n","\n","Consider a $k$-armed bandit problem with $k=4$ actions, denoted 1, 2, 3, and 4. \n","\n","Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a)=0$, for all $a$. \n","\n","Suppose the initial sequence of actions and rewards is $A_1=1$, $R_1=-1$, $A_2=2$, $R_2=1$, $A_3=2$, $R_3=-2$, $A_4=2$, $R_4=2$, $A_5=3$, $R_5=0$. \n","\n","On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. \n","\n","1) On which time steps did this definitely occur? \n","\n","2) On which time steps could this possibly have occurred?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qgQwMI3ooqwb"},"source":["### Answer:\n","\n","Build a table for $Q_t(a)$ for each time step $t$:\n","\n","|     | a=1 | a=2 | a=3 | a=4 |\n","|-----|-----|-----|-----|-----|\n","| t=1 | 0   | 0   | 0   | 0   |\n","| t=2 | -1  | 0   | 0   | 0   |\n","| t=3 | -1  | 1   | 0   | 0   |\n","| t=4 | -1  | -0.5| 0   | 0   |\n","| t=5 | -1  | 0.33| 0   | 0   |\n","\n","- $A_1=1$: random or greedy\n","- $A_2=2$: random or greedy\n","- $A_3=2$: random or greedy\n","- $A_4=2$: definitely $\\epsilon$\n","- $A_5=3$: definitely $\\epsilon$"]},{"cell_type":"markdown","metadata":{"id":"YGjFhqrkkBXD","colab_type":"text"},"source":["### Exercise 2.3\n","\n","In the comparison shown in Figure 2.2, which method will performIn the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."]},{"cell_type":"markdown","metadata":{"id":"dpkiRoTXXkwG","colab_type":"text"},"source":["## 2.4 Incremental Implementation\n","\n","Let $R_i$ denote the reward received after the $i$th selection of an action, and let $Q_n$ denote the estimate of its action value after it has been selected $n-1$ times, where\n","\n","$$\n","Q_n \\doteq \\frac{R_1 + … + R_{n-1}}{n-1}.\n","$$\n","\n","If we record all rewards then computate whenever the estimated value was needed, memory and computational requirements would grow over time. \n","\n","Instead, do\n","\n","$$\n","Q_{n+1} = \\frac{1}{n} \\sum_{i=1}^{n} R_i\n","= …\n","= Q_n + \\frac{1}{n}[R_n - Q_n]\n","$$\n","\n","Requires memory only for $Q_n$ and $n$ and the small computation for each new reward.\n","\n","General form:\n","\n","$$\n","NewEstimate \\leftarrow OldEstimate + Stepsize  [ Target - OldEstimate ]\n","$$\n","\n","$[Target - OldEstimate]$ is the _error_ in the estimate. \n","Denote step-size parameter by $\\alpha$ or $\\alpha_t(a)$\n"]},{"cell_type":"markdown","metadata":{"id":"V5ZDxL9hYArj","colab_type":"text"},"source":["## 2.5 Tracking a Nonstationary Problem\n","\n","Averaging methods so far are appropriate for stationary bandit problems — reward probabilities do not change over time. For non stationary problems, makes more sense to give more weight to recent rewards than to long-past rewards. Can do this by using a constant step-size parameter, e.g. $\\alpha \\in (0, 1]$\n","\n","$$\n","Q_{n+1} = Q_n + \\alpha[R_n - Q_n]\n","…\n","= (1 - \\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^{n - i} R_i.\n","$$\n","\n","Results in $Q_{n+1}$ being a weighted average of past rewards and initial estimate $Q_1$ since sum of the weights is $(1 - \\alpha)^n + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^(n - i) = 1$. Weight given to $R_i$ decays exponentially."]},{"cell_type":"markdown","metadata":{"id":"nNEPAD_bYLs0","colab_type":"text"},"source":["## 2.6 Optimistic Initial Values \n","\n","Methods so far are dependent on initial action-value estimates, $Q_1(a)$ — _biased_ by their initial estimates. For sample-average methods, bias disappears once all actions have been selected at least once, but for methods with constant $\\alpha$, the bias not usually a problem and can be helpful. \n","\n","Initial action values can be used to encourage exploration by setting the initial estimate far from the true mean. Can be effective on stationary problems but not well-suited to non stationary problems. Any methods that focuses on initial conditions is unlikely to help with nonstationary problems."]},{"cell_type":"markdown","metadata":{"id":"lMACKIo7YUpm","colab_type":"text"},"source":["## 2.7 Upper-Confidence-Bound Action Selection\n","\n","Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. \n","\n","In $\\epsilon$-greedy action selection, would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. Can select actions according to \n","\n","$$\n","A_t \\doteq \\argmax_a \\left[ Q_t(a) + c \\sqrt{\\frac{ln(t)}{N_t(a)}} \\right] \n","$$\n","\n","- $N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$\n","- $c$ > 0 controls the degree of exploration \n","- If $N_t(a) = 0$ then $a$ is considered to be a maximising action\n","\n","This _upper confidence bound_ (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value. $c$ determines the confidence level on the possible true value of action $a$. Each time $a$ is elected, uncertainty is presumably reduced. \n","\n","UCB often performs well on bandits, but is more difficult than $\\epsilon$-greedy to extend to the more general reinforcement learning settings."]},{"cell_type":"markdown","metadata":{"id":"NLGuSwT0YYG-","colab_type":"text"},"source":["## 2.8 Gradient Bandit Algorithms\n","\n","Consider a numerical _preference_ for each action $a$, which we denote $H_t(a)$ in R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important. \n","\n","Let $\\pi_t(a)$ be the probability of taking action $a$ at time $t$.\n","\n","$$\n","Pr\\{A_t = a\\} \\doteq \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k} e^{H_t(b)}} \\doteq \\pi_t(a)\n","$$\n","\n","Initially all action preferences are the same so that all actions have equal probability of being selected \n","\n","Without reward baseline term and gradient bandit algorithm, performance degraded. "]},{"cell_type":"markdown","metadata":{"id":"FNb0eVlqYbEf","colab_type":"text"},"source":["## 2.9 Associative Search (Contextual bandits)\n","\n","So far considered only non associative tasks. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is non-stationary. In a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. \n","\n","An _associative search_ task (contextual bandits) involves both trial-and-error learning to _search_ for the best actions, and _association_ of these actions with the situations in which they are best. Associative search tasks are intermediate between $k$-armed bandit problem and full reinforcement learning problem. Full reinforcement learning problem when each action affects immediate reward and _next situation_."]},{"cell_type":"markdown","metadata":{"id":"Drz5s50JYdsP","colab_type":"text"},"source":["## 2.10 Summary\n","\n","UCB methods choose deterministically but achieve exploration by subtly favouring at each step the actions that have so far received fewer samples. \n","\n","Gradient bandit algorithms estimate not action values, but action preferences, and favour them ore preferred actions in a graded, probabilistic manner using a soft-max distribution. "]}]}