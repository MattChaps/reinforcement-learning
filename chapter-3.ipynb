{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter-3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNRI+dXqfFgxn1WOebk9z41"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cwMsCj7zDfOd","colab_type":"text"},"source":["# Chapter 3 \n","\n","# Finite Markov Decision Processes\n","\n","The problem of finite MDPs involves evaluative feedback, as in bandits, but also an associative aspect &mdash; choosing different actions in different situations. They are a classical formalisation of seqeunential decisions making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards."]},{"cell_type":"markdown","metadata":{"id":"lh6Uel85Di6n","colab_type":"text"},"source":["## 3.1 The Agent-Environment Interface\n","\n","MDPs are meant to be a straightforward framing of the problem of learning from interaction to acheive a goal. \n","\n","![](https://drive.google.com/thumbnail?id=1GRXp8d1oNqq3vCJ6TqiAQWVr9VtXnf6P)\n","\n","The MDP and agent together give rise to a sequence or _trajectory_ like this:\n","\n","$$\n","S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots .\n","$$\n","\n","In a _finite_ MDP, the sets of states, actions, and rewards ($\\mathcal{S}$, $\\mathcal{A}$, and $\\mathcal{R}$) all have a finite number of elements. \n","\n","Given $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$:\n","\n","$$\n","p(s', r | s, a) \\doteq Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a \\},\n","$$\n","\n","for all $s' \\in \\mathcal{S}$, $r \\in \\mathcal{R}$, and $a \\in \\mathcal{A(s)}$.*\n","\n","_*$\\mathcal{A(s)}$ since action selected based on state $s$_ "]},{"cell_type":"markdown","metadata":{"id":"85DQwkCNoRFG","colab_type":"text"},"source":["The function _p_ defines the _dynamics_ of the MDP. "]},{"cell_type":"markdown","metadata":{"id":"b1HDfmgKpdmu","colab_type":"text"},"source":["_State-transition probabilities_, $p : \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$, where\n","\n","$$\n","p(s' | s, a) \\doteq Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a \\} = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a).\n","$$\n","\n","_Expected rewards for state-action pairs_, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, where\n","\n","$$\n","r(s, a) \\doteq \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a).\n","$$\n","\n","Expected rewards for state-action-next-state triples, $r : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$,\n","\n","$$\n","r(s, a, s') \\doteq \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r | s, a)}{p(s' | s, a)}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kR28dXU607uW","colab_type":"text"},"source":["Sensory receptors of an agent should be considered part of the environment rather than part of the agent. Rewards, too, are computed inside the artificial learning system but are considered external to the agent. "]},{"cell_type":"markdown","metadata":{"id":"O5jGPS4-1QXE","colab_type":"text"},"source":["Anything that cannot be changed arbitrarily by the agent is considered to be outside of it and this part of its environment. The agent-environment boundary represents the limit of the agent's _absolute control_, not of its knowledge "]},{"cell_type":"markdown","metadata":{"id":"rnpTVPXt-Gij","colab_type":"text"},"source":["## 3.3 Returns and Episodes\n","\n","__Return__:\n","\n","$$\n","G_t \\doteq R_{t+1} + R_{t+2} + \\dots + R_{T},\n","\\tag{3.7}\n","$$\n","\n","where $T$ is a final time step.\n","\n","__Episodes__: natural subsequences of agent-environemnt interaction, e.g. plays of a game.\n","\n","__Terminal state__: special state that ends an episode. This is followed by a reset to a standard starting state.\n","\n","__Episode task__: tasks with episodes that all end in the same terminal state, with different rewards for the different outcomes.\n","\n","__$\\mathcal{S}$__: set of all nonterminal states\n","\n","__$\\mathcal{S}^+$__: set of all states plus the terminal state\n","\n","__Discounted return__:\n","\n","$$\n","\\begin{align*}\n","G_t & \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\n","& = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1},\n","\\end{align*}\n","\\tag{3.8}\n","$$\n","\n","where $\\gamma$, $0 \\leq \\gamma \\leq 1$, is the _discount rate_.\n","\n","__Discount rate__: parameter that determines the present value of future rewards.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SUmived34FVL","colab_type":"text"},"source":["## 3.5 Policies and Value Functions \n","\n","The _value function_ of a state $s$ under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_\\pi$ formally by\n","\n","$$\n","v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_T | S_t = s] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\middle| S_t = s \\right], \\text{for all $s \\in \\mathcal{S}$,} \n","$$\n","\n","where $\\mathbb{E}_\\pi[\\cdot]$ denotes the expected value of a random variable given that the agent follows policy $\\pi$, and $t$ is any time step. Note that the value of the terminal state, if any, is always zero. We call the function $v_\\pi$ the _state-value function for policy $\\pi$_.\n","\n","Define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_\\pi(s, a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$:\n","\n","$$\n","q_\\pi(s, a) \\doteq \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right].\n","\\tag{3.13}\n","$$\n","\n","Call $q_\\pi$ the _action-value function for policy $\\pi$_."]},{"cell_type":"markdown","metadata":{"id":"Vygi3QE92FGB","colab_type":"text"},"source":["For any policy $\\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value o its possible successor states: (The _Bellman equation for $v_\\pi$_)\n","\n","$$\n","\\begin{align*}\n","v_\\pi(s) & \\doteq \\mathbb{E}_\\pi \\left[ G_T \\middle| S_t = s \\right] \\\\\n","& = \\sum_{a} \\pi \\left( a \\middle| s \\right) \\sum_{s', r} p \\left( s', r \\middle| s, a \\right) \\left[ r + \\gamma v_\\pi(s') \\right], \\text{ for all $s \\in \\mathcal{S}$,} \n","\\tag{3.14}\n","\\end{align*}\n","$$\n","\n","The equation states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. "]},{"cell_type":"markdown","metadata":{"id":"x_RX_Enp7mLH","colab_type":"text"},"source":["### Example 3.6: Golf\n","\n","- Reward: -1 for each stroke until we hit the ball into the hole.\n","- State: location of the ball.\n","- Value of a state: negative of the number of strokes to the hole from that location.\n","- Actions: how we aim and swing at the ball and which club we select."]}]}