{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter-3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNXiVLVhglWxii2covTy5zj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cwMsCj7zDfOd","colab_type":"text"},"source":["# Chapter 3 \n","\n","# Finite Markov Decision Processes\n","\n","The problem of finite MDPs involves evaluative feedback, as in bandits, but also an associative aspect &mdash; choosing different actions in different situations. They are a classical formalisation of seqeunential decisions making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards."]},{"cell_type":"markdown","metadata":{"id":"lh6Uel85Di6n","colab_type":"text"},"source":["## 3.1 The Agent-Environment Interface\n","\n","MDPs are meant to be a straightforward framing of the problem of learning from interaction to acheive a goal. \n","\n","![](https://drive.google.com/thumbnail?id=1GRXp8d1oNqq3vCJ6TqiAQWVr9VtXnf6P)\n","\n","The MDP and agent together give rise to a sequence or _trajectory_ like this:\n","\n","$$\n","S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots .\n","$$\n","\n","In a _finite_ MDP, the sets of states, actions, and rewards ($\\mathcal{S}$, $\\mathcal{A}$, and $\\mathcal{R}$) all have a finite number of elements. \n","\n","Given $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$:\n","\n","$$\n","p(s', r | s, a) \\doteq Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a \\},\n","$$\n","\n","for all $s' \\in \\mathcal{S}$, $r \\in \\mathcal{R}$, and $a \\in \\mathcal{A(s)}$.*\n","\n","_*$\\mathcal{A(s)}$ since action selected based on state $s$_ "]},{"cell_type":"markdown","metadata":{"id":"85DQwkCNoRFG","colab_type":"text"},"source":["The function _p_ defines the _dynamics_ of the MDP. "]},{"cell_type":"markdown","metadata":{"id":"b1HDfmgKpdmu","colab_type":"text"},"source":["_State-transition probabilities_, $p : \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$, where\n","\n","$$\n","p(s' | s, a) \\doteq Pr\\{S_t = s' | S_{t-1} = s, A_{t-1} = a \\} = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a).\n","$$\n","\n","_Expected rewards for state-action pairs_, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, where\n","\n","$$\n","r(s, a) \\doteq \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a).\n","$$\n","\n","Expected rewards for state-action-next-state triples, $r : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$,\n","\n","$$\n","r(s, a, s') \\doteq \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r | s, a)}{p(s' | s, a)}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kR28dXU607uW","colab_type":"text"},"source":["Sensory receptors of an agent should be considered part of the environment rather than part of the agent. Rewards, too, are computed inside the artificial learning system but are considered external to the agent. "]},{"cell_type":"markdown","metadata":{"id":"O5jGPS4-1QXE","colab_type":"text"},"source":["Anything that cannot be changed arbitrarily by the agent is considered to be outside of it and this part of its environment. The agent-environment boundary represents the limit of the agent's _absolute control_, not of its knowledge "]}]}