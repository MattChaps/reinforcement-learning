## 2.1 A k-armed Bandit Problem

- Consider the learning problem:

  - faced repeatedly with choice among k different options;
  - after each choice receive numerical reward chosen from a stationary probability distribution that depends on the action you selected

- Objective is to maximise expected total reward over some number of time steps.

- This is original form of k-armed _bandit problem_, so named by analogy to a slot machine with k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and rewards are payoffs for hitting jackpot. Through repeated action selections you are to maximise your winnings by concentrating your actions on the best levers.   

- Each of the k actions has an expected or mean reward given that that actions is selected -- _value_ of that action.

- Let action selected on time step _t_ as A<sub>t</sub>, and the corresponding reward as R<sub>t</sub>. The value then of an arbitrary action _a_, denoted q<sub>*</sub>(a), is the expected reward given that _a_ is selected:

<img src="https://render.githubusercontent.com/render/math?math=
q_*(a) \doteq \mathbb{E}[R_t|A_t=a]">

- Assume we do not know the action values with certainty, although may have estimates. Denote the estimated value of action _a_ at time step _t_ as Q<sub>t</sub>(a). We would like Q<sub>t</sub>(a) to be close to q<sub>*</sub>(a).

- If maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. Call these the _greedy_ actions. When select one of these actions, say _exploiting_ current knowledge of the values of the actions. Otherwise, _exploring_.

- Whether it is better to explore or exploit depends on precise values of estimates, uncertainties, and number of remaining steps.

- In this book do not worry about balancing exploration and exploitation in sophisticated way; worry only about balancing them at all
