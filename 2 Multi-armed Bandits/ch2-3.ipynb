{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-zZH573-agc"
   },
   "source": [
    "## 2.3 The 10-armed Testbed\n",
    "\n",
    "To assess relative effectiveness of greedy and $\\epsilon$-greedy action-value methods, compare them numerically on test problems.\n",
    "\n",
    "Compare greedy method with two $\\epsilon$-greedy methods ($\\epsilon=0.01$ and $\\epsilon=0.1$).\n",
    "\n",
    "For Average Reward against Steps, greedy improved slightly faster in the beginning, but then leveled off at a lower level. The $\\epsilon$-greedy methods eventually performed better because they continued to explore.\n",
    "\n",
    "Advantage of $\\epsilon$-greedy over greedy methods depends on the task. For example, with noisier rewards it takes more exploration to find the optimal action, and $\\epsilon$-greedy methods should fare even better relative to the greedy method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7d9XZaI-oq_I"
   },
   "source": [
    "### Exercise 2.2: Bandit example\n",
    "\n",
    "Consider a $k$-armed bandit problem with $k=4$ actions, denoted 1, 2, 3, and 4. \n",
    "\n",
    "Consider applying to this problem a bandit algorithm using $\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a)=0$, for all $a$. \n",
    "\n",
    "Suppose the initial sequence of actions and rewards is $A_1=1$, $R_1=-1$, $A_2=2$, $R_2=1$, $A_3=2$, $R_3=-2$, $A_4=2$, $R_4=2$, $A_5=3$, $R_5=0$. \n",
    "\n",
    "On some of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at random. \n",
    "\n",
    "1) On which time steps did this definitely occur? \n",
    "\n",
    "2) On which time steps could this possibly have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgQwMI3ooqwb"
   },
   "source": [
    "### Answer:\n",
    "\n",
    "Build a table for $Q_t(a)$ for each time step $t$:\n",
    "\n",
    "|     | a=1 | a=2 | a=3 | a=4 |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| t=1 | 0   | 0   | 0   | 0   |\n",
    "| t=2 | -1  | 0   | 0   | 0   |\n",
    "| t=3 | -1  | 1   | 0   | 0   |\n",
    "| t=4 | -1  | -0.5| 0   | 0   |\n",
    "| t=5 | -1  | 0.33| 0   | 0   |\n",
    "\n",
    "- $A_1=1$: random or greedy\n",
    "- $A_2=2$: random or greedy\n",
    "- $A_3=2$: random or greedy\n",
    "- $A_4=2$: definitely $\\epsilon$\n",
    "- $A_5=3$: definitely $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "In the comparison shown in Figure 2.2, which method will performIn the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMXoCjOLqf3w6WFLiIBma1n",
   "collapsed_sections": [],
   "name": "ch2-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
