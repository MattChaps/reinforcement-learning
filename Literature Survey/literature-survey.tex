\documentclass{article}

\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\addbibresource{references.bib}

\title{Reinforcement Learning: A Literature Survey}
\author{Matthew Chapman}

\begin{document}
\maketitle

\section{Introduction}
Reinforcement learning is the class of problems concerned with an agent learning behaviour through trial-and-error interactions with a dynamic environment \cite{Kaelbling1996}.

An example is an aspiring tightrope artist (the agent) learns to walk
from one end of a rope to another without falling (the behaviour) by
repeatedly correcting their balance (the trial-and-error interactions) whenever the rope wobbles beneath them (the dynamic environment).

In this literature survey we: 
\begin{enumerate}
  \item{describe the problems we want to solve with reinforcement learning, and explain why they are interesting to solve;} 
  \item{describe historical and current work in the field, including the kinds of problems solved and the approaches; as well as,} 
  \item{state what we will be using in our own project.}
\end{enumerate}

Much of the environments mentioned in this literature survey are of games, as they (board games in particular) are ideal testing grounds for exploring concepts and approaches in reinforcement learning and artificial intelligence \cite{Tesauro1995}.

\section{Motivation}
Reinforcement learning agents exhibit behaviour similar to intelligence, with intelligence defined as ``adaptation with insufficient knowledge and resources'' \cite{OnDefiningArtificialIntelligence}. It is also said that humans and other animals use reinforcement learning to understand their environment and generalise past experience to new situations \cite{Mnih2015}. To study reinforcement learning, therefore, is to study a small area of exhibiting human intelligence in machines, and solving reinforcement learning problems, such as those done in robotics or chip design, can lead to developments that improve quality of life and ultimately contribute to the search for AGI \cite{Ipek2008, Kober2013}.

In the following section, we start off by introducing constraints to restrict ourselves to ``simple'' environments with well-defined rewards that are easier to solve mathematically. 

\section{Markov Decision Processes}

To formalise this idea, we phrase the process of ``learning'' in terms of an optimisation problem. The framework in which this is done is that of Markov Decision Processes (MDPs).

The Markov structure means that $$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, S_2, \dots, S_t),$$ which is that the next state only depends on the current state of the environment the agent is in. 

An example of an environment which possesses the Markov property is a Chess board: the next move a player can make depends only on the current board state, disregarding how the game was played up to that point. 

This is important because we do not need to store any history, making it easier for the agent to determine an optimal behaviour. In fact, given perfect information, there are exactly solvable equations for the optimal solution \cite{}.

Together, the agent and MDP give rise to a trajectory like this:

\begin{enumerate}
  \item{At time $t$, the agent senses the state of its environment, $S_t$.}
  \item{The agent selects an action, $A_t$, which affects the environment.}
  \item{The environment is changed, now at time $t+1$, and returns a reward signal, $R_{t+1}$, to the agent, and the process is repeated.}
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/fig-3.1.png}
  \caption{The agent-environment interaction in a Markov decision process \cite{Sutton1998}.}
\end{figure}

The agent selects actions dictated by its policy, $\pi$, and the goal of the agent is to maximise the total reward it receives from the environment over time.

To maximise the total reward over time, the agent must consider more than just immediate rewards. The \textbf{return} $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},$$ is the sum of (discounted) future rewards, where $\gamma \in [0,1]$ is the discount rate. This allows us to value states by a \textbf{value function} $$v_\pi(s) \doteq \mathbb{E}_\pi \left[ G_T \middle| S_t = s \right],$$ which is the expected return when starting in $s$ and following behaviour, or policy, $\pi$ thereafter, and actions by an \textbf{action-value function} $$q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a],$$ which is the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$.

The best possible policy under these assumptions is given by the solution, $q_*$ to the Bellman optimality equation, $$ q_*(s,a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a') \middle| S_t=s, A_t=a \right].$$ 

However, the solution relies on three assumptions that in reality can rarely be achieved:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the dynamics of the environment are accurately known;
\item
  computational resources are sufficient to complete the calculation; and,
\item
  the states have the Markov property.
\end{enumerate}

Usable RL methods provide ways of approximating solutions to $q_*$ even when lacking complete information or having restricted computational resources. We will consider such methods in the next sections.

\section{Dynamic Programming}

DISCUSS DP in the context of some papers that use it 

Dynamic programming (DP) refers to the collection of algorithms
that can be used to compute optimal policies given a perfect model of
the environment as a MDP. Even in the case of tasks with continuous state and action spaces, the spaces can be discretised and finite-state DP methods applied to obtain approximate solutions.

DP iteratively evaluates then improves a policy based on the value function of each state; therefore, an imperfect model, such as one where a state has a negative reward despite it being positive in reality, will propagate the error and cause the algorithm to converge to a "wrong" optimal policy. 

\textbf{Iterative policy evaluation:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initial approximation, \(v_0\), is chosen arbitrarily.
\item
  Each successive approximation is obtained by using the Bellman
  equation for \(v_\pi\) as an update rule.
\end{enumerate}

\textbf{Policy improvement:} the process of making a new policy that
improves on an original policy, by making it greedy with respect to the
value function of the original policy

\textbf{Value iteration}

\[
\begin{aligned}
  v_{k+1}(s)
  & \doteq \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \middle| S_t = s, A_t = a \right] 
  \\
  & = \max_a \sum_{s', r} p \left( s', r \middle| s, a \right) \left[ r + \gamma v_k(s') \right], 
  \quad \text{ for all $s \in \mathcal{S}$}
\end{aligned}
\]

DP methods involve operations over the entire state set of the MDP. If
the state set is very large, like in Backgammon, then each set is
expensive. Therefore, DP is not useful for very big state spaces. In order to approximate things, Monte Carlo methods can be employed, which is covered in the next section.

There also exists \emph{Asynchronous} DP algorithms, which update the value of states in any order whatsoever, using whatever values of other states happen to be
available.

Policy iteration consists of making the value function consistent with
the current policy (policy evaluation), and the other making the policy
greedy with respect to the current value function (policy improvement).
The result is convergence to the optimal value function and an optimal
policy.

\section{Monte Carlo Methods}

To estimate the value of a state from experience is to
average the returns observed after visits to that state. As more returns
are observed, the average should converge to the expected value. This is the basis of Monte Carlo (MC) methods for learning the state-value function for a given policy.

Defining a visit to \(s\) as each occurrence of state \(s\) in an episode, there are two main MC methods:

\textbf{First-visit MC method:} estimates \(v_\pi(s)\) as the average of
the returns following first visits to s.

\textbf{Every-visit MC method:} averages the returns following all
visits to \(s\).

Both methods converge to \(v_\pi(s)\) as the number of visits (or first
visits) to \(s\) goes to infinity.

There are three advantages of Monte Carlo methods over DP methods:

\begin{itemize}
\tightlist
\item
  The computational expense of estimating the value of a single state is
  independent of the number of states;
\item
  The ability to learn from actual experience; and,
\item
  The ability to learn from simulated experience.
\end{itemize}

If a model is not available, then it is particularly useful to estimate
\emph{action} values rather than \emph{state} values. With a model,
state values alone are sufficient to determine a policy. One of our
primary goals for Monte Carlo methods is to estimate \(q_*\). To achieve
this, we first consider the policy evaluation problem for action values.

The policy evaluation problem for action values is to estimate
\(q_\pi(s,a)\). The MC methods for this are essentially the same as just
presented for state values, replacing state with state-action pair.

The complication is that many state-action pairs may never be visited.
To compare alternatives we need to estimate the value of \emph{all} the
actions from each state. One way to do this is by specifying that the episodes \emph{start in a state-action pair}, and that every pair has a non-zero probability of being selected at the start.

Monte Carlo methods can be used to find optimal policies given only
sample episodes and no other knowledge of the environment's dynamics.

Since the assumption of exploring starts is unlikely, the only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches:

\textbf{On-policy methods:} evaluate or improve the policy that is used
to make decisions.

\textbf{Off-policy methods} evaluate or improve a policy different from
that used to generate the data.

All learning control methods face a dilemma: they seek to learn action
values conditional on subsequent \emph{optimal} behaviour, but they need
to behave non-optimally in order to explore all actions (to \emph{find}
the optimal actions). The on-policy approach is a compromise --- it
learns action values not for the optimal policy, but for a near-optimal
policy that still explores. A more straightforward approach is to use
two policies, one that is learned about and that becomes the optimal
policy, and one that is more exploratory and is used to generate
behaviour.

\textbf{Target policy:} the policy being learned

\textbf{Behaviour policy:} the policy used to generate behaviour

Learning is from data ``off'' the target policy, and the overall process
is termed \emph{off-policy} learning.

Off-policy methods utilise \emph{importance sampling}

\textbf{Importance sampling:} technique for estimating expected values
under one distribution given samples from another.

Apply importance sampling to off-policy learning by weighting returns
according to the relative probability of their trajectories occurring
under the target and behaviour policies, called the
\emph{importance-sampling ratio}.

The importance-sampling ratio depends only on the two policies and the
sequence, not on the MDP.

There are two types of importance sampling: ordinary, and weighted.

Ordinary importance sampling is unbiased whereas weighted importance
sampling is biased (though the bias converges asymptotically to zero).

Monte Carlo prediction methods can be implemented incrementally, on an
episode-by-episode basis. In Monte Carlo methods we average
\emph{returns}.

An advantage is that the target policy may be greedy, while the
behaviour policy can continue to sample all possible actions.

These methods follow the behaviour policy while learning about and
improving the target policy.

\section{Temporal-Difference Learning}

Temporal-difference (TD) learning gets the best of both DP methods and MC methods. Like MC methods, TD methods can learn directly from raw experience without requiring a model of the environment (it is model-free), or iterating over every state. Also like DP methods, TD methods can update their estimates without requiring the final outcome. 

The main difference from MC methods is that we learn (update the value function) from incomplete episodes. Instead, we take a partial trajectory and estimate how much reward you think you'll get in place of the actual return (bootstrapping).

In TD(0) (the simplest version of TD-learning), we update our value function $V(S_t)$ towards an estimate of the return: $$V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)),$$ where the TD error $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$.

\section{Function Approximation}

For environments where there are too many states or actions to fit into memory, such as Backgammon with $10^{20}$ states or Go with $10^{170}$, which are too slow to process, we must estimate the value function: $$\hat{v}(s, \mathbf{w}) \simeq v_\pi(S).$$ For control, we'd do: $$\hat{q}(S, A, \mathbf{w}) \simeq q_\pi(S, A).$$

Having replaced the value function with a function approximator, how do you train it? 

\section{Stochastic-gradient Methods}

Stochastic-gradient descent (SDG) methods are learning methods for function approximation in value prediction. They are well suited to online reinforcement learning (update our estimate of the value function at every step without waiting until the episode finishes), where data becomes available in sequential order and is used to update the best predictor at each step \cite{Mnih2015}.

In gradient-descent methods, the \textit{approximate value function} $\hat{v}(s,\mathbf{w})$ is a differentiable function of a \textit{weight vector} $\mathbf{w} \doteq (w_1, w_2, \dots, w_d)^T$ for all $s \in \mathcal{S}$. $\mathbf{w}$ gets updated at each discrete $t = 0, 1, 2, 3, \dots$ with each new observation, and $\mathbf{w}_t$ denotes the weight vector at each step. We try to minimise the \textit{Mean Squared Value Error}, denoted $\overline{VE}$: $$\overline{VE} \doteq \sum_{s \in \mathcal{S}} \mu(s) \left[ v_\pi(s) - \hat{v}(s, \mathbf{w})\right].$$

SDG methods minimise error on the observed samples by adjusting the weight vector after each vector by a small amount in the direction that would most reduce the error on that example: $$\begin{aligned} \mathbf{w}_{t+1} & \doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla_{\mathbf{w}} \left[ v_\pi(S_t) - \hat{v}(S_t, \mathbf{w}_t) \right]^2 \\ & = \mathbf{w}_t + \alpha(v_\pi(S_t) - \hat{v}(S_t, \mathbf{w}_t)) \nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w}_t),\end{aligned}$$ where $\alpha$ is a positive step-size parameter.

In order to compute the ground-truth value function, $v_\pi(S_t)$, we substitute it with a target.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/substituting-v.png}
  \caption{Slide taken from Chris Wilcock's presentation on function approximation.}
  \label{fig:substituting-v}
\end{figure}

To learn the optimal behaviour so we have our agents learn the best way to play a game, we need the action-value function, which we approximate exactly the same as for the value function, with $\hat{q}(S, A, \mathbf{w}) \simeq q_\pi(S,A)$  

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/substituting-q.png}
  \caption{Slide taken from Chris Wilcock's presentation on function approximation.}
  \label{fig:substituting-q}
\end{figure}

Since incremental methods (learning from each experience) are not sample efficient and lead to rapidly forgetting rare experiences that would be useful alter on, we introduce a experience replay, or replay buffer. This stores experiences (such as the last 50k experiences) and reuses them. These can be sampled uniformly or prioritised to replay important transitions more frequently.

\section{Deep Reinforcement Learning}

Deep reinforcement learning is the field of research that combines reinforcement learning and deep learning. It has allowed machines to solve more complex decision-making tasks than ever before, due to its usefulness in problems with high dimensional state-space and/or low prior knowledge \cite{DBLP:journals/corr/abs-1811-12560} For example, a deep RL agent can learn from inputs made up of pixels to play Atari 2600 games by using a deep convolutional neural network to approximate the optimal action-value function \cite{Mnih2015}. Deep RL comes with its own challenges, and so a variety of deep RL algorithms have been developed.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/deep-rl-algorithms.png}
  \caption{A taxonomy of deep reinforcement learning algorithms \cite{openai_2018}.}
  \label{fig:deep-rl-algorithms}
\end{figure}

The first branching point in the tree is to decide whether an RL algorithm is model-free or model-based. The algorithm is model-based if the agent has access to (or learns) a model of the environment; that is, there is a function which predicts state transitions and rewards. Otherwise, it is model-free.

Model-free methods are then divided into two families: policy optimisation, and Q-learning.

\section{Policy Optimisation}

Policy optimisation methods represent a policy explicitly as $\pi_\theta(a|s)$ and optimise the parameters $\theta$ either directly by gradient ascent on $J(\pi_\theta)$, the expected return when the agent acts according to it, or indirectly, by maximising local approximations of $J(\pi_\theta)$. They also involve learning an approximator for the on-policy value function, which gets used in figuring out how to update the policy \cite{openai_2018}.

\subsection{Intro To Policy Optimisation}

Consider a stochastic, parametrised policy, $\pi_\theta$. We aim to maximise the expected return $J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]$, where $R(\tau) = \sum_{t=0}^Tr_t$ gives the finite-horizon undiscounted return, the sum of rewards obtained in a fixed window of steps. The mechanism policy gradient algorithms use to optimise the policy is gradient ascent, e.g. $$\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta) |_{\theta_k},$$ where $\nabla_\theta J(\pi_\theta)$ is called the policy gradient.

To use this function, we need an expression for the policy gradient which we can numerically compute. This involves two steps:
\begin{enumerate}
  \item deriving the analytical gradient of policy performance, which has the form of an expected value, and then
  \item forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.
\end{enumerate}

The simplest expression for the policy gradient, derived here \cite{}, is $$\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right].$$ This is an expected which we can estimate with a sample mean $$\hat{g} = \frac{1}{\lvert \mathcal{D} \rvert} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau),$$ where $\lvert \mathcal{D} \rvert = \{ \tau_i \}_{i=1, \dots, N}$ is the number of trajectories in $\mathcal{D}$ (here, $N$).

Taking a step with this gradient, however, pushes up the log-probabilities of each action in proportion to the sum of \textit{all rewards ever obtained}. We want to only reinforce actions based on rewards obtained after they are taken, by $$\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) \right].$$

Furthermore, to reduce variance in the sample estimate for the policy gradient (resulting in faster and more stable learning), we can subtract the on-policy value function $V^\pi(s)$ (which cannot be computed exactly, so has to be approximated), which gives the expected return if we start in state $s$ and always act according to policy $/pi$, from the expression without changing it in expectation by the GLP lemma: $$\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t'=t}^T R \left( s_{t'}, a_{t'}, s_{t'+1} - b(s_t)\right) \right].$$ 

\subsection{Vanilla Policy Gradient}

The key idea of policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return, until you arrive at the optimal policy.

Vanilla policy gradient (VPG) is an on-policy algorithm that can be used for environments with either discrete or continuous action spaces. The method is susceptible to the policy getting trapped in local optima.

\subsection{Trust Region Policy Optimisation}

Trust region policy optimisation (TRPO) updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms of KL-Divergence, a measure of distance between probability distributions.

The theoretical TRPO update is: $$\theta_{k+1} = arg \max_\theta \mathcal{L}(\theta_k, \theta) \text{ s.t. } \overline{D}_{KL}(\theta || \theta_k) \leq \delta,$$ where $$\mathcal{L}(\theta_k, \theta) = \mathbb{E}_{s, a \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a) \right],$$ and $$ \overline{D}_{KL}(\theta||\theta_k) = \mathbb{E}_{s \sim \pi_{\theta_k}} \left[ D_{KL}(\pi_\theta(\cdot|s) || \pi_{\theta_k}(\cdot|s)) \right].$$ In practice, the objective and constraint are expanded around $\theta_k$, resulting in an approximate optimisation problem. The approximate problem can then be analytically solved by the methods of Lagrangian duality; in addition, TRPO modifies the update rule by adding a backtracking line search.  

\subsection{Proximal Policy Optimisation}

Proximal policy optimization (PPO) algorithms alternate between sampling data through interaction with the environment, and optimising a ``surrogate'' objective function using stochastic gradient ascent. Unlike standard policy gradient methods, multiple epochs of mini-batch updates are possible instead of performing one gradient update per data sample \cite{DBLP:journals/corr/SchulmanWDRK17}.

Similar to TRPO, PPO tries to take the biggest possible improvement step on a policy using the data we currently have while avoiding performance collapse. PPO is a family of first-order methods that are significantly simpler to implement, and perform at least as well as TRPO.

The PPO variant we consider is PPO-Clip: it does not have a KL-divergence term in the objective and does not have a constraint at all; instead, it relies on specialised clipping in the objective function to remove incentives for the new policy to get far fro the old policy.

PPO-clip updates policies via $$\theta_{k+1} = arg \max_\theta \mathbb{E}_{s,a \sim \pi_{\theta_k}} \left[ L(s,a,\theta_k,\theta) \right],$$ typically taking multiple steps of (minibatch) SDG to maximise the objective. $L$ is given by $$L(s,a,\theta_k,\theta) = min \left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), \text{clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}}(s,a) \right),$$ in which $/epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old. Gradient clipping is a technique to prevent exploding gradients, such as rescaling gradients so that their norm is at most a particular value. 

A simplified version is as follows: $$L(s,a,\theta_k,\theta) = min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \right),$$ where $$g(\epsilon, A) = \begin{cases} (1+\epsilon)A & \mbox{if } A \geq 0 \\ (1-\epsilon)A & \mbox{if } A < 0. \end{cases}$$

To prevent ending up with a new policy which is too far from the old policy, we can stop taking the gradient steps if the mean KL-divergence of the new policy form the old grows beyond a threshold (early stopping). The method is susceptible to policies being trapped in local optima.

\section{Q-Learning}
Q-learning is a value-based class of algorithms that aim to build a value function, which subsequently lets us define a policy.

$Q$-learning keeps a lookup table of values $Q(s,a)$ with one entry for every state-action pair. In order to learn the optimal $Q$-value function, the $Q$-learning algorithm makes use of the Bellman equation for the $Q$-value function whose unique solution is $Q^*(s,a)$ \cite{DBLP:journals/corr/abs-1811-12560}.

However, convergence to the optimal value function assumes that the state-action pairs are represented discretely, and all actions are repeatedly sampled in all states; this is often inapplicable with a high-dimensional state-action space.

\section{Long Short-Term Memory (LSTM) \cite{RLLSTM}}

Among the more important tasks for RL are tasks where part of the state of the environment is \textit{hidden} from the agent. The agent now not only needs to learn the mapping from environmental states to actions, for optimal performance it needs to determine which environmental state it is in as well.

Long-term dependencies allow the agent to distinguish between identical states in its environment, such as T-junctions in a maze, by remembering observations or actions before reaching the state. LSTM is a solution to learning long-term dependencies in time series data.

LSTM is a RNN architecture that solves the problem that conventional RNN algorithms have with errors propagating back in time tending to either vanish or blow up.

RNNs can be applied to RL tasks by letting it learn a model of the environment. LSTM architecture would allow the predictions of observations and rewards to depend on information from long ago. The model-based system could then learn the mapping from environmental states to actions using standard techniques such as Q-learning. Alternatively, the RNN could directly approximate the value function of a RL algorithm.

\section{Our Project}

In this project, we want to look at deep reinforcement learning algorithms, implement them, and analyse their performance in those environments provided by OpenAI Gym.

\printbibliography
\end{document}