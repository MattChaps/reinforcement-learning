\documentclass{article}

\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\addbibresource{references.bib}

\title{Reinforcement Learning: A Literature Survey}
\author{Matthew Chapman}

\begin{document}
\maketitle

\section{Introduction}
Reinforcement learning is the class of problems concerned with an agent learning behaviour through trial-and-error interactions with a dynamic environment \cite{Kaelbling1996}.

An example is an aspiring tightrope artist (the agent) learns to walk
from one end of a rope to another without falling (the behaviour) by
repeatedly correcting their balance (the trial-and-error interactions) whenever the rope wobbles beneath them (the dynamic environment).

In this literature survey we: 
\begin{enumerate}
  \item{describe the problems we want to solve with reinforcement learning, and explain why they are interesting to solve;} 
  \item{describe historical and current work in the field, including the kinds of problems solved and the approaches; as well as,} 
  \item{state what we will be using in our own project.}
\end{enumerate}

Much of the environments mentioned in this literature survey are of games, as they (board games in particular) are ideal testing grounds for exploring concepts and approaches in reinforcement learning and artificial intelligence \cite{Tesauro1995}.

\section{Motivation}
Reinforcement learning agents exhibit behaviour similar to intelligence, with intelligence defined as ``adaptation with insufficient knowledge and resources'' \cite{OnDefiningArtificialIntelligence}. It is also said that humans and other animals use reinforcement learning to understand their environment and generalise past experience to new situations \cite{Mnih2015}. To study reinforcement learning, therefore, is to study a small area of exhibiting human intelligence in machines, and solving reinforcement learning problems, such as those done in robotics \cite{Kober2013} or chip design  \cite{Ipek2008}, can lead to developments that improve quality of life and ultimately contribute to the search for AGI.

In the following section, we start off by introducing constraints to restrict ourselves to "simple" environments with well-defined rewards that are easier to solve mathematically. 

\section{Markov Decision Processes}

The classical framework used to abstract a real problem as a mathematically idealised reinforcement learning problem is the Markov Decision Process (MDP).

Together, the agent and MDP give rise to a trajectory like this:

\begin{enumerate}
  \item{At time $t$, the agent senses the state of its environment, $S_t$.}
  \item{The agent selects an action, $A_t$, which affects the environment.}
  \item{The environment is changed, now at time $t+1$, and returns a reward signal, $R_{t+1}$, to the agent, and the process is repeated.}
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/fig-3.1.png}
  \caption{The agent-environment interaction in a Markov decision process \cite{Sutton1998}.}
\end{figure}

The agent selects actions dictated by its policy, $\pi$, and the goal of the agent is to maximise the total reward it receives from the environment over time.

In order to maximise the total reward over time, the agent must consider more than just immediate rewards.

The \textbf{return} $G_t$ is defined as the discounted total future reward, 

\[ 
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+l+1},
\]

where $\gamma$ determines the weight (or importance) given to rewards at later time-steps.

The \textbf{value function of a state $s$ under a policy $\pi$} is the total reward an agent can expect to accumulate, starting from $s$ and following $\pi$ thereafter:

\[
\begin{aligned}
v_\pi(s) \doteq \mathbb{E}_\pi \left[ G_T \middle| S_t = s \right].
\end{aligned}
\]

Every state must satisfy this condition.

The \textbf{value function of taking action \(a\) in state \(s\) under a policy \(\pi\)},

\[
\begin{aligned}
q_\pi(s, a) & \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \\
& = \mathbb{E} \left[ \sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \middle| S_t = s, A_t = a \right],
\end{aligned}
\]

is the expected return starting from \(s\), taking the action \(a\), and
thereafter following policy \(\pi\).

Solving the \textbf{Bellman optimality equation for \(q_*\)},

\[\begin{aligned} 
q_*(s,a) & = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a') \middle| S_t=s, A_t=a \right], 
\end{aligned}\]

provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. However, the solution relies on at
least three assumptions that are rarely true in practice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the dynamics of the environment are accurately known;
\item
  computational resources are sufficient to complete the calculation; and,
\item
  the states have the Markov property.
\end{enumerate}

Section 5 highlights methods that circumvent this.

\section{Dynamic Programming}

Dynamic programming (DP) refers to the collection of algorithms
that can be used to compute optimal policies given a perfect model of
the environment as a Markov decision process (MDP).

We usually assume that the environment is a finite MDP. A common way of
obtaining approximate solutions for tasks with continuous state and
actions is to quantise the state and action spaces and then apply
finite-state DP methods.

The key idea of DP, and of reinforcement learning generally, is the use
of value functions to organise and structure the search for good
policies.

How do you compute the state-value function \(v_\pi\) for an arbitrary policy \(\pi\)? If the environment's dynamics are completely known, then iterative solution methods are most suitable.

\textbf{Iterative policy evaluation:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initial approximation, \(v_0\), is chosen arbitrarily.
\item
  Each successive approximation is obtained by using the Bellman
  equation for \(v_\pi\) as an update rule.
\end{enumerate}

\textbf{Policy improvement:} the process of making a new policy that
improves on an original policy, by making it greedy with respect to the
value function of the original policy

\textbf{Value iteration}

\[
\begin{aligned}
  v_{k+1}(s)
  & \doteq \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \middle| S_t = s, A_t = a \right] 
  \\
  & = \max_a \sum_{s', r} p \left( s', r \middle| s, a \right) \left[ r + \gamma v_k(s') \right], 
  \quad \text{ for all $s \in \mathcal{S}$}
\end{aligned}
\]

DP methods involve operations over the entire state set of the MDP. If
the state set is very large, like in Backgammon, then each set is
expensive.

\emph{Asynchronous} DP algorithms update the value of states in any
order whatsoever, using whatever values of other states happen to be
available.

Policy iteration consists of making the value function consistent with
the current policy (policy evaluation), and the other making the policy
greedy with respect to the current value function (policy improvement).
The result is convergence to the optimal value function and an optimal
policy.

\section{Monte Carlo Methods}

To estimate the value of a state from experience is to
average the returns observed after visits to that state. As more returns
are observed, the average should converge to the expected value. This is the basis of Monte Carlo (MC) methods for learning the state-value function for a given policy.

Defining a visit to \(s\) as each occurrence of state \(s\) in an episode, there are two main MC methods:

\textbf{First-visit MC method:} estimates \(v_\pi(s)\) as the average of
the returns following first visits to s.

\textbf{Every-visit MC method:} averages the returns following all
visits to \(s\).

Both methods converge to \(v_\pi(s)\) as the number of visits (or first
visits) to \(s\) goes to infinity.

There are three advantages of Monte Carlo methods over DP methods:

\begin{itemize}
\tightlist
\item
  The computational expense of estimating the value of a single state is
  independent of the number of states;
\item
  The ability to learn from actual experience; and,
\item
  The ability to learn from simulated experience.
\end{itemize}

If a model is not available, then it is particularly useful to estimate
\emph{action} values rather than \emph{state} values. With a model,
state values alone are sufficient to determine a policy. One of our
primary goals for Monte Carlo methods is to estimate \(q_*\). To achieve
this, we first consider the policy evaluation problem for action values.

The policy evaluation problem for action values is to estimate
\(q_\pi(s,a)\). The MC methods for this are essentially the same as just
presented for state values, replacing state with state-action pair.

The complication is that many state-action pairs may never be visited.
To compare alternatives we need to estimate the value of \emph{all} the
actions from each state. One way to do this is by specifying that the episodes \emph{start in a state-action pair}, and that every pair has a nonzero probability of being selected at the start.

Monte Carlo methods can be used to find optimal policies given only
sample episodes and no other knowledge of the environment's dynamics.

Since the assumption of exploring starts is unlikely, the only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches:

\textbf{On-policy methods:} evaluate or improve the policy that is used
to make decisions.

\textbf{Off-policy methods} evaluate or improve a policy different from
that used to generate the data.

All learning control methods face a dilemma: they seek to learn action
values conditional on subsequent \emph{optimal} behaviour, but they need
to behave non-optimally in order to explore all actions (to \emph{find}
the optimal actions). The on-policy approach is a compromise --- it
learns action values not for the optimal policy, but for a near-optimal
policy that still explores. A more straightforward approach is to use
two policies, one that is learned about and that becomes the optimal
policy, and one that is more exploratory and is used to generate
behaviour.

\textbf{Target policy:} the policy being learned

\textbf{Behaviour policy:} the policy used to generate behaviour

Learning is from data ``off'' the target policy, and the overall process
is termed \emph{off-policy} learning.

Off-policy methods utilise \emph{importance sampling}

\textbf{Importance sampling:} technique for estimating expected values
under one distribution given samples from another.

Apply importance sampling to off-policy learning by weighting returns
according to the relative probability of their trajectories occurring
under the target and behaviour policies, called the
\emph{importance-sampling ratio}.

The importance-sampling ratio depends only on the two policies and the
sequence, not on the MDP.

There are two types of importance sampling: ordinary, and weighted.

Ordinary importance sampling is unbiased whereas weighted importance
sampling is biased (though the bias converges asymptotically to zero).

Monte Carlo prediction methods can be implemented incrementally, on an
episode-by-episode basis. In Monte Carlo methods we average
\emph{returns}.

An advantage is that the target policy may be greedy, while the
behaviour policy can continue to sample all possible actions.

These methods follow the behaviour policy while learning about and
improving the target policy.

\section{Deep Reinforcement Learning}

Deep reinforcement learning is the field of research that combines reinforcement learning and deep learning. It has allowed machines to solve more complex decision-making tasks than ever before, due to its usefulness in problems with high dimensional state-space and/or low prior knowledge \cite{DBLP:journals/corr/abs-1811-12560} For example, a deep RL agent can learn from inputs made up of pixels to play Atari 2600 games by using a deep convolutional neural network to approximate the optimal action-value function \cite{Mnih2015}. Deep RL comes with its own challenges, and so a variety of deep RL algorithms have been developed.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/deep-rl-algorithms.png}
  \caption{A taxonomy of deep reinforcement learning algorithms \cite{openai_2018}.}
  \label{fig:deep-rl-algorithms}
\end{figure}

The first branching point in the tree is to decide whether an RL algorithm is model-free or model-based. The algorithm is model-based if the agent has access to (or learns) a model of the environment; that is, there is a function which predicts state transitions and rewards. Otherwise, it is model-free.

Model-free methods are then divided into two families: policy optimisation, and Q-learning.

\section{Policy Optimisation \cite{openai_2018}} 

Policy optimisation methods represent a policy explicitly as $\pi_\theta(a|s)$ and optimise the parameters $\theta$ either directly by gradient ascent on $J(\pi_\theta)$, the expected return when the agent acts according to it, or indirectly, by maximising local approximations of $J(\pi_\theta)$. They also involve learning an approximator for the on-policy value function, which gets used in figuring out how to update the policy.

\section{Q-Learning}
Q-learning is a value-based class of algorithms that aim to build a value function, which subsequently lets us define a policy.

$Q$-learning keeps a lookup table of values $Q(s,a)$ with one entry for every state-action pair. In order to learn the optimal $Q$-value function, the $Q$-learning algorithm makes use of the Bellman equation for the $Q$-value function whose unique solution is $Q^*(s,a)$ \cite{DBLP:journals/corr/abs-1811-12560}.

However, convergence to the optimal value function assumes that the state-action pairs are represented discretely, and all actions are repeatedly sampled in all states; this is often inapplicable with a high-dimensional state-action space.

\section{Long Short-Term Memory (LSTM) \cite{RLLSTM}}

Among the more important tasks for RL are tasks where part of the state of the environment is \textit{hidden} from the agent. The agent now not only needs to learn the mapping from environmental states to actions, for optimal performance it needs to determine which environmental state it is in as well.

Long-term dependencies allow the agent to distinguish between identical states in its environment, such as T-junctions in a maze, by remembering observations or actions before reaching the state. LSTM is a solution to learning long-term dependencies in timeseries data.

LSTM is a RNN architecture that solves the problem that conventional RNN algorithms have with errors propagating back in time tending to either vanish or blow up.

RNNs can be applied to RL tasks by letting it learn a model of the environment. LSTM's architecture would allow the predictions of observations and rewards to depend on information from long ago. The model-based system could then learn the mapping from environmental states to actions using standard techniques such as Q-learning. Alternatively, the RNN could directly approximate the value function of a RL algorithm.

\section{Our Project}

In this project, we want to look at deep reinforcement learning algorithms, implement them, and analyse their performance in those environments provided by OpenAI Gym.

\printbibliography
\end{document}