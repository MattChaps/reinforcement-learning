\documentclass{article}

\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\addbibresource{references.bib}

\title{Reinforcement Learning: A Literature Survey}
\author{Matthew Chapman}

\begin{document}
\maketitle

\section{Introduction}
Reinforcement learning is the class of problems concerned with an agent learning behaviour through trial-and-error interactions with a dynamic environment \cite{Kaelbling1996}.

An example is an aspiring tightrope artist (the agent) learns to walk
from one end of a rope to another without falling (the behaviour) by
repeatedly correcting their balance (the trial-and-error interactions) whenever the rope wobbles beneath them (the dynamic environment).

In this literature survey we: 
\begin{enumerate}
  \item{describe the problems we want to solve with reinforcement learning, and explain why they are interesting to solve;} 
  \item{describe historical and current work in the field, including the kinds of problems solved and the approaches; as well as,} 
  \item{state what we will be using in our own project.}
\end{enumerate}

Much of the environments mentioned in this literature survey are of games, as they (board games in particular) are ideal testing grounds for exploring concepts and approaches in reinforcement learning and artificial intelligence \cite{Tesauro1995}.

\section{Motivation}
Reinforcement learning agents exhibit behaviour similar to intelligence, with intelligence defined as ``adaptation with insufficient knowledge and resources'' \cite{OnDefiningArtificialIntelligence}. It is also said that humans and other animals use reinforcement learning to understand their environment and generalise past experience to new situations \cite{Mnih2015}. To study reinforcement learning, therefore, is to study a small area of exhibiting human intelligence in machines, and solving reinforcement learning problems, such as those done in robotics \cite{Kober2013} or chip design  \cite{Ipek2008}, can lead to developments that improve quality of life and ultimately contribute to the search for AGI.

In the following section, we start off by introducing constraints to restrict ourselves to ``simple'' environments with well-defined rewards that are easier to solve mathematically. 

\section{Markov Decision Processes}

To formalise this idea, we phrase the process of ``learning'' in terms of an optimisation problem. The framework in which this is done is that of Markov Decision Processes (MDPs).

The Markov structure means that $$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, S_2, \dots, S_t),$$ which is that the next state only depends on the current state of the environment the agent is in. 

An example of an environment which possesses the Markov property is a Chess board: the next move a player can make depends only on the current board state, disregarding how the game was played up to that point. 

This is important because we do not need to store any history, making it easier for the agent to determine an optimal behaviour. In fact, given perfect information, there are exactly solvable equations for the optimal solution \cite{}.

Together, the agent and MDP give rise to a trajectory like this:

\begin{enumerate}
  \item{At time $t$, the agent senses the state of its environment, $S_t$.}
  \item{The agent selects an action, $A_t$, which affects the environment.}
  \item{The environment is changed, now at time $t+1$, and returns a reward signal, $R_{t+1}$, to the agent, and the process is repeated.}
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/fig-3.1.png}
  \caption{The agent-environment interaction in a Markov decision process \cite{Sutton1998}.}
\end{figure}

The agent selects actions dictated by its policy, $\pi$, and the goal of the agent is to maximise the total reward it receives from the environment over time.

To maximise the total reward over time, the agent must consider more than just immediate rewards. The \textbf{return} $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},$$ is the sum of (discounted) future rewards, where $\gamma \in [0,1]$ is the discount rate. This allows us to value states by a \textbf{value function} $$v_\pi(s) \doteq \mathbb{E}_\pi \left[ G_T \middle| S_t = s \right],$$ which is the expected return when starting in $s$ and following behaviour, or policy, $\pi$ thereafter, and actions by an \textbf{action-value function} $$q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a],$$ which is the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$.

The best possible policy under these assumptions is given by the solution, $q_*$ to the Bellman optimality equation, $$ q_*(s,a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1},a') \middle| S_t=s, A_t=a \right].$$ 

However, the solution relies on at least three assumptions that are rarely true in practice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the dynamics of the environment are accurately known;
\item
  computational resources are sufficient to complete the calculation; and,
\item
  the states have the Markov property.
\end{enumerate}

Usable RL methods provide ways of approximating solutions to $q_*$ even when lacking complete information or having restricted computational resources. We will consider such methods in the next sections.

\section{Dynamic Programming}

DISCUSS DP in the context of some papers that use it 

Dynamic programming (DP) refers to the collection of algorithms
that can be used to compute optimal policies given a perfect model of
the environment as a MDP. Even in the case of tasks with continuous state and action spaces, the spaces can be discretised and finite-state DP methods applied to obtain approximate solutions.

DP iteratively evaluates then improves a policy based on the value function of each state; therefore, an imperfect model, such as one where a state has a negative reward despite it being positive in reality, will propagate the error and cause the algorithm to converge to a "wrong" optimal policy. 

\textbf{Iterative policy evaluation:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initial approximation, \(v_0\), is chosen arbitrarily.
\item
  Each successive approximation is obtained by using the Bellman
  equation for \(v_\pi\) as an update rule.
\end{enumerate}

\textbf{Policy improvement:} the process of making a new policy that
improves on an original policy, by making it greedy with respect to the
value function of the original policy

\textbf{Value iteration}

\[
\begin{aligned}
  v_{k+1}(s)
  & \doteq \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \middle| S_t = s, A_t = a \right] 
  \\
  & = \max_a \sum_{s', r} p \left( s', r \middle| s, a \right) \left[ r + \gamma v_k(s') \right], 
  \quad \text{ for all $s \in \mathcal{S}$}
\end{aligned}
\]

DP methods involve operations over the entire state set of the MDP. If
the state set is very large, like in Backgammon, then each set is
expensive. Therefore, DP is not useful for very big state spaces. In order to approximate things, Monte Carlo methods can be employed, which is covered in the next section.

There also exists \emph{Asynchronous} DP algorithms, which update the value of states in any order whatsoever, using whatever values of other states happen to be
available.

Policy iteration consists of making the value function consistent with
the current policy (policy evaluation), and the other making the policy
greedy with respect to the current value function (policy improvement).
The result is convergence to the optimal value function and an optimal
policy.

\section{Monte Carlo Methods}

To estimate the value of a state from experience is to
average the returns observed after visits to that state. As more returns
are observed, the average should converge to the expected value. This is the basis of Monte Carlo (MC) methods for learning the state-value function for a given policy.

Defining a visit to \(s\) as each occurrence of state \(s\) in an episode, there are two main MC methods:

\textbf{First-visit MC method:} estimates \(v_\pi(s)\) as the average of
the returns following first visits to s.

\textbf{Every-visit MC method:} averages the returns following all
visits to \(s\).

Both methods converge to \(v_\pi(s)\) as the number of visits (or first
visits) to \(s\) goes to infinity.

There are three advantages of Monte Carlo methods over DP methods:

\begin{itemize}
\tightlist
\item
  The computational expense of estimating the value of a single state is
  independent of the number of states;
\item
  The ability to learn from actual experience; and,
\item
  The ability to learn from simulated experience.
\end{itemize}

If a model is not available, then it is particularly useful to estimate
\emph{action} values rather than \emph{state} values. With a model,
state values alone are sufficient to determine a policy. One of our
primary goals for Monte Carlo methods is to estimate \(q_*\). To achieve
this, we first consider the policy evaluation problem for action values.

The policy evaluation problem for action values is to estimate
\(q_\pi(s,a)\). The MC methods for this are essentially the same as just
presented for state values, replacing state with state-action pair.

The complication is that many state-action pairs may never be visited.
To compare alternatives we need to estimate the value of \emph{all} the
actions from each state. One way to do this is by specifying that the episodes \emph{start in a state-action pair}, and that every pair has a non-zero probability of being selected at the start.

Monte Carlo methods can be used to find optimal policies given only
sample episodes and no other knowledge of the environment's dynamics.

Since the assumption of exploring starts is unlikely, the only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches:

\textbf{On-policy methods:} evaluate or improve the policy that is used
to make decisions.

\textbf{Off-policy methods} evaluate or improve a policy different from
that used to generate the data.

All learning control methods face a dilemma: they seek to learn action
values conditional on subsequent \emph{optimal} behaviour, but they need
to behave non-optimally in order to explore all actions (to \emph{find}
the optimal actions). The on-policy approach is a compromise --- it
learns action values not for the optimal policy, but for a near-optimal
policy that still explores. A more straightforward approach is to use
two policies, one that is learned about and that becomes the optimal
policy, and one that is more exploratory and is used to generate
behaviour.

\textbf{Target policy:} the policy being learned

\textbf{Behaviour policy:} the policy used to generate behaviour

Learning is from data ``off'' the target policy, and the overall process
is termed \emph{off-policy} learning.

Off-policy methods utilise \emph{importance sampling}

\textbf{Importance sampling:} technique for estimating expected values
under one distribution given samples from another.

Apply importance sampling to off-policy learning by weighting returns
according to the relative probability of their trajectories occurring
under the target and behaviour policies, called the
\emph{importance-sampling ratio}.

The importance-sampling ratio depends only on the two policies and the
sequence, not on the MDP.

There are two types of importance sampling: ordinary, and weighted.

Ordinary importance sampling is unbiased whereas weighted importance
sampling is biased (though the bias converges asymptotically to zero).

Monte Carlo prediction methods can be implemented incrementally, on an
episode-by-episode basis. In Monte Carlo methods we average
\emph{returns}.

An advantage is that the target policy may be greedy, while the
behaviour policy can continue to sample all possible actions.

These methods follow the behaviour policy while learning about and
improving the target policy.

\section{Stochastic-gradient Methods}

Stochastic-gradient descent (SDG) methods are learning methods for function approximation in value prediction. They are well suited to online reinforcement learning, where data becomes available in sequential order and is used to update the best predictor at each step \cite{Mnih2015}.

In gradient-descent methods, the \textit{approximate value function} $\hat{v}(s,\mathbf{w})$ is a differentiable function of a \textit{weight vector} $\mathbf{w} \doteq (w_1, w_2, \dots, w_d)^T$ for all $s \in \mathcal{S}$. $\mathbf{w}$ gets updated at each discrete $t = 0, 1, 2, 3, \dots$ with each new observation, and $\mathbf{w}_t$ denotes the weight vector at each step. We try to minimise the \textit{Mean Squared Value Error}, denoted $\overline{VE}$: $$\overline{VE} \doteq \sum_{s \in \mathcal{S}} \mu(s) \left[ v_\pi(s) - \hat{v}(s, \mathbf{w})\right]$$.

SDG methods minimise error on the observed samples by adjusting the weight vector after each vector by a small amount in the direction that would most reduce the error on that example: $$\mathbf{w}_{t+1} \doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \left[ v_\pi(S_t) - \hat{v}(S_t, \mathbf{w}_t)\right]^2,$$ where $\alpha$ is a positive step-size parameter.

\section{Proximal Policy Optimisation}

Proximal policy optimization (PPO) algorithms are a family of policy gradient methods for reinforcement learning that alternate between sampling data through interaction with the environment, and optimising a ``surrogate'' objective function using stochastic gradient ascent. Unlike standard policy gradient methods, multiple epochs of mini-batch updates are possible instead of performing one gradient update per data sample \cite{DBLP:journals/corr/SchulmanWDRK17}.

\section{Deep Reinforcement Learning}

Deep reinforcement learning is the field of research that combines reinforcement learning and deep learning. It has allowed machines to solve more complex decision-making tasks than ever before, due to its usefulness in problems with high dimensional state-space and/or low prior knowledge \cite{DBLP:journals/corr/abs-1811-12560} For example, a deep RL agent can learn from inputs made up of pixels to play Atari 2600 games by using a deep convolutional neural network to approximate the optimal action-value function \cite{Mnih2015}. Deep RL comes with its own challenges, and so a variety of deep RL algorithms have been developed.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/deep-rl-algorithms.png}
  \caption{A taxonomy of deep reinforcement learning algorithms \cite{openai_2018}.}
  \label{fig:deep-rl-algorithms}
\end{figure}

The first branching point in the tree is to decide whether an RL algorithm is model-free or model-based. The algorithm is model-based if the agent has access to (or learns) a model of the environment; that is, there is a function which predicts state transitions and rewards. Otherwise, it is model-free.

Model-free methods are then divided into two families: policy optimisation, and Q-learning.

\section{Policy Optimisation \cite{openai_2018}} 

Policy optimisation methods represent a policy explicitly as $\pi_\theta(a|s)$ and optimise the parameters $\theta$ either directly by gradient ascent on $J(\pi_\theta)$, the expected return when the agent acts according to it, or indirectly, by maximising local approximations of $J(\pi_\theta)$. They also involve learning an approximator for the on-policy value function, which gets used in figuring out how to update the policy.

\section{Q-Learning}
Q-learning is a value-based class of algorithms that aim to build a value function, which subsequently lets us define a policy.

$Q$-learning keeps a lookup table of values $Q(s,a)$ with one entry for every state-action pair. In order to learn the optimal $Q$-value function, the $Q$-learning algorithm makes use of the Bellman equation for the $Q$-value function whose unique solution is $Q^*(s,a)$ \cite{DBLP:journals/corr/abs-1811-12560}.

However, convergence to the optimal value function assumes that the state-action pairs are represented discretely, and all actions are repeatedly sampled in all states; this is often inapplicable with a high-dimensional state-action space.

\section{Long Short-Term Memory (LSTM) \cite{RLLSTM}}

Among the more important tasks for RL are tasks where part of the state of the environment is \textit{hidden} from the agent. The agent now not only needs to learn the mapping from environmental states to actions, for optimal performance it needs to determine which environmental state it is in as well.

Long-term dependencies allow the agent to distinguish between identical states in its environment, such as T-junctions in a maze, by remembering observations or actions before reaching the state. LSTM is a solution to learning long-term dependencies in time series data.

LSTM is a RNN architecture that solves the problem that conventional RNN algorithms have with errors propagating back in time tending to either vanish or blow up.

RNNs can be applied to RL tasks by letting it learn a model of the environment. LSTM architecture would allow the predictions of observations and rewards to depend on information from long ago. The model-based system could then learn the mapping from environmental states to actions using standard techniques such as Q-learning. Alternatively, the RNN could directly approximate the value function of a RL algorithm.

\section{Our Project}

In this project, we want to look at deep reinforcement learning algorithms, implement them, and analyse their performance in those environments provided by OpenAI Gym.

\printbibliography
\end{document}