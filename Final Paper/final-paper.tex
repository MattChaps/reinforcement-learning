\documentclass[12pt,a4paper]{article}

\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{amssymb,amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfig}
\graphicspath{ {./images/} }

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Reinforcement Learning and \\ Transfer Learning Between Games}
\author{} % leave; your name goes into \student{}
\student{Matthew Chapman}
\supervisor{Dr Lawrence Mitchell}
\degree{BSc Natural Sciences}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
{\bf Background}: The recent success of reinforcement learning systems solving visually complex tasks, such as playing video games, means that such systems have potential for real-world applications, such as self-driving cars. To improve these systems, there is ongoing research into developing ways to apply knowledge gained from previous tasks to solve new tasks, such as from driving in a simulation to driving in the real world.

{\bf Aims}: The aim of this project is to develop an understanding of reinforcement learning and its intersection with transfer learning by investigating the use of deep neural networks and pre-training. We aim to assess to what extent a current state-of-the-art architecture benefits from pre-training in the context of learning to play Atari video-games.

{\bf Method}: Classic-control and Atari 2600 environments provided by OpenAI Gym are used to train an initial implementation of the proximal policy optimisation (PPO) algorithm followed by a subsequent implementation which uses a deep convolutional neural network.

{\bf Results}: 

{\bf Conclusions}: Transfer learning is a promising method to prepare agents for environments where it has limited opportunities to interact with and learn from. By training agents on similar environments, we can build confidence that the agent will perform successfully when evaluated on the test environment. This is particularly useful in fields such as autonomous driving, where the vehicle must be able to adapt to various changes in the environment.
\end{abstract}

\begin{keywords}
Artificial intelligence, machine learning, reinforcement learning, deep learning, transfer learning, game-playing.
\end{keywords}

\newpage
\section{Introduction}
This project is about reinforcement learning and transfer learning. The project involves developing a reinforcement learning algorithm to learn to successfully play Atari games. Additionally, the project involves investigating how transfer learning lets the algorithm store knowledge and apply it --- to learn to successfully play a different but related Atari game.  

\subsection{Background} 
\subsubsection{Reinforcement learning}
Reinforcement learning is the class of problems concerned with an agent learning behaviour through trial-and-error interactions with a dynamic environment \cite{Kaelbling1996}. An example of a problem is an aspiring tightrope walker (the agent) learning to maintain balance (the behaviour) while walking along a tightrope that contorts and wobbles under their weight (the dynamic environment). With each attempt and fall (the trial-and-error interactions), the walker learns how better to correct their balance, and adjusts their behaviour slightly for the next attempt. When the walker is able to maintain balance consistently over consecutive attempts, the desired behaviour is achieved, and so the learning task is complete. We say that the problem is solved and the reinforcement learning agent has learned to perform successfully in the environment.  

There are algorithms that act as agents that solve reinforcement learning problems. These reinforcement learning algorithms can solve problems in physical environments, such as driving cars, or in virtual environments, such as playing video games. We can treat these algorithms as functions that take as input observations of the environment's \emph{state}, and produce as output \emph{actions}. Examples of states are the pixel values and positions in each frame of a car video stream or video game. Examples of corresponding actions are to brake or to press a controller button. The goal of the algorithm is to learn which actions are the best to take given the observed current state. To measure how good an action is from a state (i.e., a \emph{state-action pair}), we can assign it a \emph{value}. The higher the value, the better the action is considered to be. For example, a self-driving car observing a red traffic light should give the action of braking the greatest value, if the desired behaviour is to drive safely. Similarly, an algorithm playing the Atari video-game Breakout observing the ball falling towards the left should give the action of moving left the greatest value, if the desired behaviour is to get a high score. The algorithm learns a mapping, from states and actions to values, to inform its decision-making. This mapping is initially unknown, but improves the more the algorithm interacts with its environment --- the same way one gets better with \emph{experience} at driving or playing video-games. For relatively complex problems, the value of a state-action pair must be estimated from the experience gathered so far. Figuring out a way of valuing actions is a key part of reinforcement learning models.

\begin{figure}[!tbp]
    \centering
    \subfloat[Breakout.]{\includegraphics[width=0.45\textwidth]{breakout.png}\label{fig:f1}}
    \hfill
    \subfloat[Pong.]{\includegraphics[width=0.45\textwidth]{pong.png}\label{fig:f2}}
    \caption{Frames from two Atari 2600 games. Breakout requires you to break bricks, and Pong requires you to beat an opponent at 2D table tennis. You control a paddle to hit a ball in both.}
\end{figure}

\subsubsection{Transfer learning}
Transfer learning is the application of knowledge gained while solving one problem to solve a different but related problem \cite{2010}. An example is an agent who has learned to walk a tightrope (solving one problem) applying their balancing ability (the knowledge gained) to learn to surf (a different but related problem). By reusing knowledge gained from solving past problems, it is expected that solving a different but related problem will be more efficient than it would be without the prior knowledge. As in the example, a tightrope walker should learn to balance on a surfboard more easily and more quickly, due to their knowledge of balancing on a rope, than someone without the same acquired knowledge of balancing. 

In transfer learning for reinforcement learning algorithms, the knowledge gained that can be applied is the agent's \emph{policy}. The policy is the set of rules that determine an agent's behaviour. An example of a poor policy is to take actions randomly. An example of a better policy might be to always take the action with greatest value. The policy of interest is one that the reinforcement learning algorithm developed itself while learning. Depending on the architecture of the algorithm, the policy could be a neural network, so that developing the policy equates to training the network. For similar games, such as Breakout and Pong, the hope is that the network learns abstractions of one game's mechanics, which can be used to make learning the other game more efficient.

\subsection{Context}
Reinforcement learning algorithms have proven to be successful at achieving at least human-level performance in some tasks. A historical example is TD-Gammon: a program that achieved a level of play just slightly below that of the top human backgammon players of the time \cite{}. Another example that is more recent is AlphaGo: a program that is claimed to be arguably the strongest Go player in history \cite{}. In the future, an example might be self-driving cars.

Existing reinforcement learning algorithms have some drawbacks. One is that they are most effective when there are no limits to the trial-and-error interactions an agent can make with its environment while learning. This is not an issue in virtual settings, but could cause problems in physical settings. For example, a Breakout agent could compute games indefinitely. On the other hand, an algorithm learning from scratch to drive a car will likely crash on its first run. For reasons such as not wanting to incur a repair cost or wasting time, it is preferable to be confident that a self-driving car has a reasonable ability to drive before testing it. To build confidence that a reinforcement learning algorithm will perform reasonably successfully in a real-world environment, we could first train the algorithm in a similar virtual environment, such as a simulation. We refer to this process as \textit{pre-training}. Transfer learning is a key component of this process, since the algorithm's subsequent success depends critically on its ability to transfer the knowledge gained from the virtual domain and apply it to the real domain. For reasons such as this, transfer learning has become a crucial technique to build better reinforcement learning systems \cite{}. 

\subsection{Aims}
The research question proposed is as follows: \textit{How much more efficiently do reinforcement learning agents with pre-training learn to play a different Atari game than those without?}. To address this research question, the objectives for this project were divided into three categories: minimum, intermediate, and advanced. 

The minimum objectives were to establish a candidate reinforcement learning algorithm from the literature which would likely learn good policies for playing Atari games, and implement the algorithm minimally. This minimal algorithm should be used to train an agent in a relatively simple environment, such as CartPole \cite{}. In CartPole, the agent is required to prevent an upright pendulum attached to a cart from falling over, and is expected to \textit{solve} the environment by getting an average reward of 195.0 over 100 consecutive trials. The purpose of this objective was to establish a strong understanding of reinforcement learning algorithms, and build a foundational model for the remainder of the project.

The intermediate objectives were to adapt the implemented reinforcement learning algorithm with a deep convolutional neural network and evaluate the benefit of pre-training by policy transfer. The adapted algorithm should be used to train two good agents for two Atari games, Breakout and Pong. Solving either environment requires maximising the final score. For Breakout and Pong, the trained agents should attain an average score that at least matches the popular benchmark of 401.2 and 18.9 respectively \cite{}. Then, the network weights from the trained Breakout agent should be used to initialise the network weights of a third agent learning Pong, or vice versa, and its performance evaluated against the other trained agent. The purpose of this objective was to understand how reinforcement learning algorithms can process pixels, and whether knowledge can be directly transferred across neural networks.

The advanced objectives were to develop a novel architecture and workflow for pre-training an agent with multiple policies, and evaluate the effectiveness of this approach. The architecture would be a generative model, and would be required to learn from 10 agents trained on 10 different Atari games using the reinforcement learning algorithm implemented earlier. The generative model would then be used to train an agent to play an 11th Atari game, and its performance evaluated against an agent without pre-training. The purpose of this objective was to extend the current state of the art of transfer learning in reinforcement learning, and provide a solution to one of OpenAI's \textit{unsolved problems} \cite{}.

\subsection{Achievements}
% We achieved the minimum and intermediate objectives of the project. We developed a minimal PPO algorithm that learned to perform successfully in the CartPole environment. By introducing a convolutional neural network (CNN) to the policy and value networks, the algorithm was able to learn to perform successfully in the environments Pong and Breakout. By reusing the weights in the network of one of the agents, another agent was trained and its performance analysed. We observed there to indeed be a benefit of pre-training. The greater the amount of data, the greater the benefit there was, as measured by three metrics: jump-start performance, accumulated rewards, and final performance. The advanced objective was not able to be completed due to the challenges brought about by external limiting factors, such as Covid-19. 

\newpage
\section{Related work}
\label{section:related-work}
There have been several academic achievements in developing reinforcement learning algorithms that address challenging sequential decision-making problems, with  potential for real-world applications. For an agent to learn a good behaviour, the agent has to make decisions in an environment to optimise a given notion of cumulative rewards. We focus on existing solutions that achieve this by value-based methods or policy gradient methods. Other approaches include those that combine the two methods, or are model-based.

There has also been similar successes in transfer learning for reinforcement learning \dots

\subsection{Value-based methods}
The value-based class of reinforcement learning algorithms aims to build a value function, which subsequently lets us define a policy. One of the simplest and most popular value-based algorithms is the Q-learning algorithm \cite{Watkins1992}. The basic version of Q-learning keeps a lookup table of values with one entry for every state-action pair. In order to learn the optimal Q-value function, the Q-learning algorithm makes use of the Bellman equation for the Q-value function \cite{R-352-PR}, which has a unique solution under certain conditions. This idea was expanded to involve approximating the Q-values from experiences gathered \cite{10.5555/2998828.2998976}, and parametrising the Q-values with a neural network whose parameters are updated by stochastic gradient descent (or a variant) by maximising a loss function \cite{10.1007/11564096_32}. A breakthrough approach used deep convolutional neural networks and a replay memory to play Atari games by learning from pixels and a wide range of past experiences; this approach was the deep Q-network (DQN) algorithm \cite{DBLP:journals/corr/MnihKSGAWR13}. More recent approaches made improvements to the DQN algorithm, such as duelling DQN \cite{DBLP:journals/corr/WangFL15} and double DQN \cite{DBLP:journals/corr/HasseltGS15}. Rainbow DQN \cite{DBLP:journals/corr/abs-1710-02298} is a combination of the many improvements to the DQN algorithm. For simple environments, a unique solution to the optimal Q-value function may be found; whereas, for complex environments, such as video games, modern approaches use neural networks to approximate the Q-values, which are then used to define a policy. 

One limitation with value-based approaches is that these types of algorithms are not well-suited to deal with large action spaces. In the Atari game Gravitar which has 18 actions, a DQN agent achieves a mean score of 306.7, compared to 2672 by a human \cite{Mnih2015}. 

\subsection{Policy gradient methods}
Policy gradient methods optimise a performance objective (typically the expected cumulative reward) by finding a good policy thanks to variants of stochastic gradient ascent with respect to the policy parameters. The gradient of the performance objective with respect to the policy parameters is called the policy gradient. The simplest policy gradient estimator is derived by using either the expected finite-horizon undiscounted return or expected infinite-horizon discounted return as the performance objective; this results in the REINFORCE algorithm \cite{Williams1992}. Deep deterministic policy gradient (DDPG) \cite{lillicrap2019continuous} extends this approach to deterministic policies to allow DQN algorithms to overcome the restriction of discrete actions. Another common approach is to use an actor-critic architecture that consists of two parts: an actor and a critic \cite{Konda2000}. The actor refers to the policy and the critic to the estimate of a value function (e.g., the Q-value function). In deep RL, both the actor and the critic can be represented by non-linear neural network function approximators \cite{mnih2016asynchronous}. More advanced approaches incorporate an advantage function to describe how much better or worse an action is than other actions on average (relative to the current policy) \cite{schulman2018highdimensional}, and use special constraints, expressed in terms of KL-Divergence \cite{kullback1951information}, on how close the new and old policies are allowed to be \cite{DBLP:journals/corr/SchulmanLMJA15}. Proximal policy optimisation (PPO) \cite{DBLP:journals/corr/SchulmanWDRK17} builds on the latter approach by relying on specialised clipping in the objective function. Typically, these approaches rely on tricks to keep new policies close to the old. 

One limitation of policy gradient methods is that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima. Additionally, even seemingly small differences in parameter space can have very large differences in performance --- so a single bad step can collapse the policy performance. This has drastic impacts on the algorithms' sample efficiency.

\subsection{Application of transfer learning }

\subsection{Generative models}

\newpage
\section{Solution (5 pages)}
\subsection{Specification}
To develop a reinforcement learning system that can successfully operate in its environment, the system must satisfy certain requirements. First, the system would initialise an environment and the agent in it. Second, a reinforcement learning algorithm would control the agent and interact with the environment by making observations and taking actions. Third, the environment would respond with a reward for each action taken by the agent. Finally, the algorithm would process the observations, actions, and rewards to iteratively improve the agent's behaviour.  

Additionally, there is another set of requirements for developing a system to pre-train reinforcement learning agents. First, the trajectories from one or more reinforcement learning agents operating in different environments would be generated. Second, a generative model would learn the agents' behaviour by predicting the actions they took. Finally, the generative model would control the agent in the new environment.  

\subsection{Tools used}
We used the Python programming language for our implementation, since Python has numerous libraries for machine learning, with support for deep learning and reinforcement learning. Examples of libraries are Keras, PyTorch, and TensorFlow. Of these, we used the PyTorch library, since this is currently what most researchers are using to implement their state-of-the-art papers. This library allowed us to implement existing architectures and easily make modifications to them, without the additional complexity of programming in low-level PyTorch. Additionally, we used OpenCV and Scikit-learn for image manipulation in our pre-processing steps.

So we wouldn't need to create our own API for Atari emulations, we used Gym \cite{1606.01540}, a toolkit developed by OpenAI for developing and comparing reinforcement learning algorithms. This allowed us to focus on implementing our reinforcement learning algorithm. Gym provides a collection of environments, including control theory problems, physics simulations, and Atari games from the Arcade Learning Environment (ALE) \cite{bellemare13arcade}. The environments have a shared interface, so we can write general algorithms that will operate in any type of environment.

For network training, we used Google Colab, which allows Python code to be written and executed in the browser. Google Colab does not require configuration and has free access to GPUs (including CUDA). There was the option to use Durham University's NVIDIA CUDA Centre (NCC), although we chose not to use it due to having no prior remote server experience or job queuing system experience (such as SLURM).

\subsection{Proximal Policy Optimisation (PPO) algorithm}
(At the moment mostly notes that were copied. Will reword.)

The proximal policy optimisation (PPO) algorithm was chosen to be implemented due to its relative simplicity and strong performance on the Atari benchmark \cite{DBLP:journals/corr/SchulmanWDRK17}. PPO is an on-policy algorithm which can be used for environments with either discrete or continuous action spaces. PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. There are two primary variants of PPO: PPO-Penalty and PPO-Clip.

\textbf{PPO-Penalty} approximately solves a KL-constrained update like TRPO, but penalizes the KL-divergence in the objective function instead of making it a hard constraint, and automatically adjusts the penalty coefficient over the course of training so that it’s scaled appropriately.

\textbf{PPO-Clip} doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.

Our implementation is of PPO-clip, which updates policies via $$\theta_{k+1} = arg \max_\theta \mathbb{E}_{s,a \sim \pi_{\theta_k}} \left[ L(s,a,\theta_k,\theta) \right],$$ takes multiple step of batch gradient descent to maximise the objective, $L$. $L$ is given by $$L(s,a,\theta_k,\theta) = min \left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), \text{clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}}(s,a) \right),$$ in which $\epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old. Gradient clipping is a technique to prevent exploding gradients, such as rescaling gradients so that their norm is at most a particular value. 

A simplified version, which we implement in our code, is as follows: $$L(s,a,\theta_k,\theta) = min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \right),$$ where $$g(\epsilon, A) = \begin{cases} (1+\epsilon)A & \mbox{if } A \geq 0 \\ (1-\epsilon)A & \mbox{if } A < 0. \end{cases}$$

When the advantage for the state-action pair is positive, the objective reduces to

\[L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 + \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a).\]

Because the advantage is positive, the objective will increase if the
action becomes more likely --- that is, if \(\pi_{\theta}(a|s)\)
increases. But the min in this term puts a limit to how \emph{much} the
objective can increase. Once
\(\pi_{\theta}(a|s) > (1+\epsilon) \pi_{\theta_k}(a|s)\), the min kicks
in and this term hits a ceiling of
\((1+\epsilon) A^{\pi_{\theta_k}}(s,a)\). Thus: the new policy
does not benefit by going far away from the old policy.

On the other hand, when the advantage for the state-action pair is negative, the objective reduces to

\[L(s,a,\theta_k,\theta) = \max\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 - \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a).\]

Because the advantage is negative, the objective will increase if the
action becomes less likely. Similar to before, the max in this term puts a limit to how much the objective can increase. Once \(\pi_{\theta}(a|s) < (1-\epsilon) \pi{\theta_k}(a|s)\), the max kicks in and this term hits a ceiling of \((1-\epsilon) A^{\pi_{\theta_k}}(s,a)\). Thus, again: the new policy does not benefit by going far away from the old policy.

Clipping serves as a regulariser by removing incentives for the policy to change dramatically, and the hyperparameter \(\epsilon\) corresponds to how far away the new policy can go from the old while still profiting the objective. 

\begin{algorithm}[H]
    \caption{PPO-Clip pseudocode}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$} 
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Update the policy by maximizing the PPO-Clip objective:
        \begin{equation*}
        \theta_{k+1} = \arg \max_{\theta} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \min\left(
            \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)}  A^{\pi_{\theta_k}}(s_t,a_t), \;\;
            g(\epsilon, A^{\pi_{\theta_k}}(s_t,a_t))
        \right),
        \end{equation*}
        typically via stochastic gradient ascent with Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Design}
\subsubsection{Main training loop}
Our PPO implementation is based on Seungeun Rho's implementation with minimal lines of code\footnote{https://github.com/seungeunrho/minimalRL}. The main training loop consists of one agent alternating between collecting experience over $t$ time-steps and performing $k$ steps of gradient descent to update network parameters. 

When collecting experience, the agent samples an action from a state with probability distribution given by the current policy network. Since we are working with environments with a discrete number of possible actions, the probability distribution the policy returns is also discrete. In PyTorch, the distribution takes the form of a tensor, with the number of elements equal to the number of actions available. Each value corresponds to the probability of selecting the action with corresponding index. For example, if the value of the element at index 0 were 0.25, then the probability of selecting action 0, which could be to move left, is 0.25. Necessarily, the probabilities must sum up to 1 each time.

When updating the network parameters, tuples of state, action, reward, and next state from the last $t$ time-steps are used. First, advantage estimates are calculated by the following generalised advantage estimation equation: $$\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + \dots + \dots + (\gamma\lambda)^{T-t+1}\delta_{T-1},$$ where $\delta_t = r_t + \gamma V(s_t+1) - V(S_t)$. This is used to calculate the PPO-Clip objective, which we subtract from the smooth L1-loss of the value function. Smooth L1-loss creates a criterion that uses a squared term if the absolute element-wise error falls below beta, and an L1 term otherwise. This is used because it is less sensitive to outliers than mean-squared error loss (squared L2 loss), and in some cases prevents exploding gradients \cite{girshick2015fast}. The loss between $x$ and $y$ is as follows: $$\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}$$ where $z_i$ is  given by the following:

\[\begin{aligned}
z_{i} =
\begin{cases}
0.5 (x_i - y_i)^2 / beta, & \text{if } |x_i - y_i| < beta \\
|x_i - y_i| - 0.5 * beta, & \text{otherwise }
\end{cases}
\end{aligned}\]

(Beta is an optional parameter that defaults to 1.)

Finally, back-propagation is performed by the Adam optimisation algorithm \cite{kingma2017adam}, which can handle sparse gradients on noisy problems.

\subsubsection{Network architecture}
We use a deep convolutional neural network as the main component of our policy gradient and value function estimators. ``There are typically 3 main types of layers that are used to build DCNN, which will be
used throughout our solution. Convolutional layers perform convolution by sliding a filter
across the image and computing the dot product of the filter with the image values at each
point, this produces a two-dimensional activation map which gives the response of the filter
at each point in the image. Many of these filters can be present in a single layer, each
producing a separate activation map. As the network is trained, it will learn filters that
activate when they see specific features within the image, such as circular patterns or vertical
edges. Convolutional layers are the backbone of the convolutional neural network. Fully-connected layers typically occur after the
convolutional layers, and have full connections to all activations in the previous
layer, similar to a layer in a basic neural network. They allow us to shape the output of the
network to fit the number of classes in the problem.''

\begin{itemize}
    \item Our network architecture consists of a deep convolutional neural network connected to two fully-connected layers on two branches. 
    \item The motivation behind this is that the DCNN learns edges and features, which the fully-connected layers then use to each estimate the value function and policy gradient respectively.
    \item Our DCNN consists of 3 blocks. 1) 4 in (of 84x84 image), 32 out, kernel size 8, stride 4. 2) 32 in, 64 out, kernel size 4, stride 2. 3) 64 in, 64 out, kernel size 3, stride 1.
    \item This is the architecture we used.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{ppo-architecture}
\caption{PPO network architecture (will create my own diagram)}
\end{figure}
    
\subsection{Implementation features}
\begin{itemize}
    \item Our implementation differs from the main paper in a number of ways.
    \item We used an architecture that shares parameters. We don't have an entropy term.
    \item We don't use early stopping to help prevent new policy being too far from old. Rule is if the mean KL-divergence of the new policy from the old grows beyond a threshold, stop taking gradient steps. The method is susceptible to policies being trapped in local optima.
    \item We did multiple steps of batch SGD rather than mini-batch SGD to maximise the objective
    \item We used our own convolutional neural network.
    \item We tried frame skipping.
    \item We did preprocessing of frames
    \item We tried resetting after end of each life. 
    \item The original algorithm anneals the adam step size and clipping parameter which we don't.
    \item The original algorithm runs 8 agents in parallel. We only did one due to hardware limitations.
\end{itemize}

\subsection{Verification}
(This might be a description of testing rather than verification.)

Verification was done on the reinforcement learning system to check that the specification was met. During developmental stages, verification methods consisted of conducting individual tests to run components of the system, followed by checking the results from the run. We tested each frame pre-processing step (greyscaling, cropping, resizing, and stacking), the convolutional layer, and both fully-connected layers, by checking the outputs were as expected. Although we did not implement verification methods for after the developmental stages, the prior tests ensured that the specification continued to be met once the system was formed. Nonetheless, an improvement would be to implement additional tests, such as to check action probabilities and replay-buffer content during training.

Were the transfer learning system to have been successfully implemented, verification would also have been done on it. For transfer learning via policy transfer, the verification method would be to check that each network parameter of the newly initialised agent was equal to that of the trained agent. For transfer learning via the generative model, verification methods would consist of conducting tests for each pre-processing step and network layer, as well as during training.

\subsection{Validation}
Validation was done to ensure that the solution would meet the project objectives. Validation methods involved discussing with the project advisor and predicting barriers that could lead to insufficient completion of the solution. This led to the updating of project objectives which were used as guidance for planning. Another validation method was to refer back to the original project proposal, \textit{Reinforcement Learning}, from which this project has evolved. By doing validation, we establish evidence that the solution achieved at least the intended minimum objectives.  

\subsection{Testing}
Unit testing, integration testing, and system testing were done on the solution.

Unit testing was done to test singular components of code to establish if it works correctly. Method, class, test case

\newpage
\section{Results}
\subsection{Evaluation method}
So that we can evaluate our reinforcement learning algorithm, we keep track of the following metrics during agent training: mean episode length, mean episode reward, amount of time elapsed, total number of time steps, objective loss, and total number of learning iterations. Additionally, so we can directly observe agent behaviour, we record a videos of an episode at regular intervals.

\subsection{Experimental settings}
\begin{itemize}
    \item These were the experimental settings for each experiment carried out: \dots
\end{itemize}

\subsection{Results}
\begin{itemize}
    \item The results generated by the software were \dots
\end{itemize}

\newpage
\section{Evaluation}
[Bridging paragraph]
\subsection{System strengths} 

\subsection{System limitations}

\subsection{Approach}
\begin{itemize}
    \item Our approach of first producing a reinforcement learning system, and then extending this to produce a transfer learning system was good. The intention was an agile approach, but this didn't really happen. A great portion of the time spent was devoted to building a theoretical foundation, a lot of which has not been mentioned.
    \item Using TensorFlow proved to be a challenge, and introduced a sharp
    learning curve. Google Colab and Gym had issues. Learning to save and load models, save videos from Gym, etc. was difficult. 
    \item The key limiting factor was the lack of experience doing project-based work and no prior background in machine learning. These had adverse effects of the quality of the solution produced.
\end{itemize}

\subsection{Project organisation}
\begin{itemize}
    \item The project was organised as well as you would expect in a global pandemic \dots
\end{itemize}

\newpage
\section{Conclusions}
\subsection{Project overview}
\begin{itemize}
    \item The project was to \dots
\end{itemize}

\subsection{Main findings}
\begin{itemize}
    \item The main findings were as follows: \dots
    \item The conclusions from these findings were \dots
\end{itemize}

\subsection{Further work}
\begin{itemize}
    \item The project can be extended by \dots
\end{itemize}

\newpage
\bibliography{references}
\end{document}